#+title: The Linux process API is bad

The Linux process API is bad.
To be fair, the Linux process API is not /uniquely/ bad.
The bad process API of Linux is shared with all the other Unix-like systems, which have the same problems.
In this article, though, I'll just be talking about Linux.

Here, when I say "process API", I am talking about the interface
for starting processes running,
monitoring processes until they exit,
and stopping process that haven't yet exited.

So what is so bad about the Linux process API?

- It makes it easy for processes to "leak", and linger on the system without anyone to force them to exit.
- Conversely, it makes it /impossible/ (without root) to guarantee that a buggy process has not leaked children.
- It uses global process identifiers (pids) instead of a secure unforgeable identifier, making it possible to confuse one process for another.
- It uses signals to communicate process events, making libraries (which can't safely handle signals) unable to spawn processes, as well as inheriting all the other problems of Unix signals.

I'll go into each of these in more depth.

* It's easy for processes to leak

What do I mean by "leak a process"?

I mean causing a process to be run without someone noting down the pid of that process,
so that process can never be killed.

Once a process has been leaked,
and you no longer have its pid or any other means to kill it,[fn:process_groups]
you either have to hope that it will exit on its own,
or manually look at what processes are running on the system and guess which ones are no longer needed.

How, exactly, does this happen?
The simplest way is when process A, starts process B, which starts process C.
If process B then exits without killing process C or somehow telling process A about C,
process C will be leaked.[fn:easyleakexample]

** How not to fix
*** Make sure B always cleans up on exit and kills C
B should just always make sure to kill process C before it exits.
That way no processes will be leaked.

What are the possible ways B might exit?

Well, B might choose to exit, possibly by throwing an exception or panicking.
In those cases, it's possible for B to kill process C immediately before exiting.

Or B might receive a signal.
B might be signaled for conventional reasons,
such as a user pressing Ctrl-C,
in which case B can still clean up, as long as the programmer or runtime take care to catch every kind of signal.

Or B might be signaled for some more unconventional reasons,
such as a segmentation fault.
It's still possible for B to clean up in this case, but it may be very tricky to do,
and the programmer or runtime may need to take great care
to make sure that the pid of C is still accessible even while handling a segfault.

Or B might receive SIGKILL.
Unfortunately, this case prevents this solution from working.
It's not possible for B to clean up when it receives SIGKILL,
so C will be unavoidably leaked.

We might want to say, "never send SIGKILL".
But that is impossible, both for a conventional reason and an ironic reason.
The conventional reason is that B might have a bug, and hang, and SIGKILL might be the only way to kill it.
The ironic reason is that the only way for B to clean up and exit in guaranteed finite time is for it to SIGKILL its own children,
so that if they have bugs they will not just hang forever.
So B would be SIGKILL'd by its own parent, implementing the same strategy.

So, in summary, it's not possible to guarantee that B cleans up and kills C.
*** Coordinate between A and B
B could carefully coordinate with A, and somehow tell A about every process B starts.
This will fail if there's a bug in B, or if B is killed just after starting a process but before telling A,
but perhaps it's good enough?

No, it's not good enough.
What if A and B come from different developers?
What if B is a normal Unix shell, which certainly isn't going to know how to do this reporting?
Are we going to convert every Unix program in existence to report back the processes they start?

Perhaps if some technique for reporting started processes was established back when Unix was first created,
that might be an acceptable solution, if ultimately flawed due to the existence of bugs and race conditions.

But it's now been 40 years since the creation of Unix,
and rewriting every Unix program unfortunately does not meet our requirement of being an "easy" way to solve this problem.
*** A should run B inside a container
If A runs B inside a container,
then no matter how many processes B starts,
A will be able to terminate them all by just stopping the container.

This has four main flaws for solving the problem of making it easy to avoid leaking.

Flaw one is that it's not easy to run a container.

Python has a "subprocess.run" function in its standard library,
for starting a subprocess.
Python has no "container.run" function in its standard library,
to start a subprocess inside a container,
and in the current container landscape that seems unlikely to change.

Shell scripts make starting processes trivial,
but it's almost unthinkable that, say, bash, would integrate functionality for starting containers,
so that every process is started in a container.
Leaving aside the issues of which container technology to use,
it would be quite complex to implement.

Flaw two is that it requires root privileges.
The syscalls required to start a Linux container require you to have root.
There are ways to get around this in recent Linux kernels[fn:user_namespaces],
but they are pretty dubious from a security perspective.

Flaw three is that it's not trivially nestable.
What if I have a hierarchy of 5 processes, ABCDE, each starting the next one in the chain?
Each one has to start its child in a new container,
but nesting containers is still tricky business that can cause breakage.

Flaw four is that it's pretty heavyweight to require literally every subprocess to run in a separate container.
Containers are heavier than normal processes,
and this will place a real burden on the system.
* It's impossible to prevent malicious processes leaks

What's a "malicious process leak"?

Well, if a "process leak" is a process existing on the system without someone knowing to kill it,
a "malicious process leak" is a process existing on the system and actively evading being killed.

A process can fork repeatedly to make a thousand copies of itself,
or just fork constantly at all times, leaving the previous processes to immediately exit,
so that its pid is constantly changing and the latest copy can't be identified and sent a signal.
A "fork bomb" is an example of an attack of this kind.

But note that this doesn't have to be the result of an attack;
simple buggy code can cause this.
If you ever program using fork(),
you could easily start forking repeatedly just from a bug.
** How not to fix
*** Run your possibly-malicious process inside a container or a virtual machine
If we run our possibly-malicious process inside a container or virtual machine,
then no matter how much it forks and exits,
we will be able to terminate the process by just stopping the container (or virtual machine).

This will actually work.
Most of our earlier concerns (it's too hard, it's heavyweight, and it's not trivially nestable)
no longer apply,
because in this section we're happy to have any means at all to prevent this attack.

However,
this solution still requires root access.
So this solution is not truly general purpose;
we can't use this routinely, every time we create a child process,
because our application certainly does not run with root access in the normal case.

We can partially get around the need for root access,
by having a privileged daemon create child processes inside a container on our behalf.[fn:systemd-run]

But having someone else fork on our behalf breaks a lot of traditional Unix features.
For example, we can't easily have our child process inherit stdin/stdout/stderr from us,
nor will it inherit environment variables or any ulimits we've placed on ourself.
So that is not ultimately workable either.
*** Limit the number of processes that can exist on the system
What if we limit the number of processes that can exist on the system,
then we kill them all one by one?
Then the process won't be able to hide.

The number of processes that can exist is actually already limited;
there's a maximum pid, and we can't have any more processes than that.
The issue is that as processes exit,
their space is usually freed up,
and new processes can be created.

So if the malicious process just keeps forking,
it can fill up the space left by previous processes exiting,
and this doesn't help us.

However, if we could prevent space from being freed up as processes exit,
the space that malicious process has to operate in would shrink and shrink,
until finally it is no longer able to fork any more, and we can kill the last copy.
Avoiding reusing process space is possible using a technique that I'll discuss at the end of this article,
which is my own solution to all the issues I'll list here.
* It uses global, reusable process IDs

A process is identified using its 'pid'.
A pid is an integer, frequently less than 65536,
which is selected for the process at startup from the pool of currently unused pids,
and which is relinquished back into that pool when the process exits.

There is a single pool of process IDs on the system.
If enough processes are started and exit,
a process ID may be reused.

Pids are mainly used to send signals to processes with the "kill" system call (which is used for any kind of signal, not just lethal ones).

Typically, a long-lived process (a "daemon") would write its own pid into a file, called a "pidfile".
Then other processes could send signals to the daemon by reading that pidfile and using "kill".

But there is absolutely not guarantee that when you "kill", you are sending a signal to the right process.
If the daemon has exited,
and enough processes have started and stopped since then,
the pid in the daemon's pidfile might point to a completely unrelated process.
You might send a fatal signal to something critically important instead of the daemon you meant to send it to!

Fundamentally, any usage of a pid is vulnerable to a [[https://en.wikipedia.org/wiki/Time_of_check_to_time_of_use][time-of-check-to-time-of-use]] race condition.
Since pids are the only way to identify a process,
this means any interaction with processes (other than your own child processes) is inherently racy.
** How not to fix
*** Only send signals to your own child processes
When process A starts process B, and then process B exits, process A is notified.
Furthermore, process B leaves a "zombie process" behind after it exits,
which consumes the pid until process A explicitly acts to get rid of the zombie process.
These two features allow process A to know exactly when it is safe to send signals to B's pid.
So if processes only send signals to their child processes,
they can send signals without races.

This works, and is an excellent replacement for pidfiles, but it is inflexible.

What if process A exits unexpectedly?
Then we are back in the situation of not being able to kill process B without a race condition.
Indeed, frequently we genuinely want process B to outlive process A;
whenever we are starting a daemon, for example.
To support this, instead of forking off a process,
process A would send a request to a supervisor daemon to start process B, as the supervisor daemon's own child.

Unfortunately, that has the same issues as discussed in the section on preventing malicious process leaks,
where we considered having a privileged daemon create containers on our behalf.
We can't easily have our child process inherit stdin/stdout/stderr from us,
nor will it inherit environment variables or any ulimits we've placed on ourself.

Furthermore, even if we have a supervisor daemon starting processes on our behalf,
this leaves a static parent-child hierarchy which cannot change.
The supervisor daemon cannot, for example, restart itself to upgrade,
as all of its child processes will stop being its children.
Nor can process A initially start up process B as process A's child,
and then later decide that process B should live past process A's exit.
*** TODO Don't reuse pids, use a UUID instead
We could identify processes with some kind of truly globally unique identifier.
Then we wouldn't have race conditions when we try to kill them,
even if they were not our own children.

This would work perfectly,
but it would be very unusual for a Unix-like system.
It would also be a fair bit slower,
though I think correctness is much more important than performance.
* It uses signals to communicate process events
Process exit is communicated to the parent of a process by SIGCHLD.
If process A starts process B, and then process B exits,
process A will be sent the SIGCHLD signal.

Signals are delivered to the entire process, and only one signal handler can be registered for each signal.

So if the main function in process A registers a signal handler for SIGCHLD,
and library L1 in process A starts a process B, when process B exits,
the signal handler of the main function in process A will receive the notification of the exit of the child,
and the library will have no idea.

Conversely, if the library L1 registers the signal handler,
and the main function or even another library L2 starts a process B,
then only L1 will be notified when the process exits.

In general, only one part of the program can directly receive signals.
That one part of the program then must forward the signal around to whatever other components desire to receive signals.
If a library has no interface for receiving signal information,
like glibc,
then it can't use child processes.

This is a major inconvenience for the library writer.
Since there's no standard interface for a library to receive signal information,
and most library writers don't want to force their users to deal with their bespoke interface,
most libraries either decide to give up the ability to use child processes,
or require that all child processes be spawned through the library.
Of course, a process can only use one library that requires all child processes be spawned through the library.
** How not to fix
*** Use signalfd
    While signalfd is certainly a great help in dealing with signals on Linux,
    it doesn't actually help deal with the problem of libraries receiving SIGCHLD.
*** TODO Chain signal handlers
    Can't we just have one library's signal handler call the next library's signal handler?

    Won't work robustly, see
    https://www.macieira.org/blog/2012/07/forkfd-part-2-finding-out-that-a-child-process-exited-on-unix/
* TODO How to fix all these problems
  Use my wrapper program, supervise, and its associate Python library!
** It's easy for processes to leak
   supervise kills all your transitive children when you exit.
** It's impossible to prevent malicious processes leaks
   supervise kills all your transitive children when you exit, securely and in a guaranteed-to-terminate way.
   (By pid exhaustion)
** It uses global, reusable, forgeable process IDs
   supervise gives you a file descriptor interface to signaling a process.
   File descriptors are local and unforgeable.
** It uses signals to communicate process events
   supervise gives you a file descriptor interface to monitor a process for exit.
   File descriptors can be monitored by libraries without interfering with the rest of the program.
* Footnotes

[fn:process_groups]
Process groups provide another means to kill a process.
And the controlling tty is yet another way.
But neither of them are fully generic and nestable.
They each allow a single additional layer of hierarchy,
where you can kill all processes in a single group,
or all processes with the same controlling tty.
But if you create a new process group while already inside some process group,
you will just leave your original process group,
and no longer be killed when that group is killed.
The same is true for the controlling tty.

TODO: I seem to recall a mailing post by the Linux cgroups maintainer,
lamenting the fact that cgroups were created as cgroups,
instead of by reusing the existing process hierarchy,
and just enforcing that it stays as a hierarchy.

[fn:systemd-run] 
systemd, for example, with its 'systemd-run' API, allows us to request that systemd start up a process for us.
systemd runs every process in a separate cgroup (which is the underlying container mechanism that we would use),
so it can protect against the malicious process leak problem.

[fn:user_namespaces]
User namespaces can be used without privileges,
but they've had a lot of vulnerabilities,
so most Linux distributions don't turn that feature on.

[fn:killcontainer] 
Is it possible to kill every process inside a container?
More concretely,
can I start a process inside a cgroup and then later kill off the process?

[fn:easyleakexample] 
Leaking a process is as simple as this:
#+BEGIN_SRC sh
sh -c '{ sleep inf & } &'
#+END_SRC
'sh' is our process A;
it forks off another copy of itself to perform the outer '&', which is our process B;
then 'sleep inf' is our process C.

