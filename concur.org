* stop writing racy garbage
don't busy-loop, instead block on connections

don't have timeouts in your test (or anywhere), timeout at the top-level
just basically don't have timeouts anywhere! they're racy!

avoid race conditions;
don't assume something happens as soon as you request it to happen;
only say "X happened" after you receive confirmation of X happening.

why long timeouts are not an issue:
if you're waiting for something happen, rather than not happen,
it won't matter.

don't sleep, for god's sake.
you might not be obviously sleeping, but you might be doing it accidentally:
if you request something and just immediately proceed as if it happened,
it *might* usually happen in time depending on how slow your code runs.
but be explicit, because that fully depends on the relative performance of your code!

(all these are "do not ever do this or you will suffer")
** run the service in parallel with other things?
hm.

well I guess this is a general principle:

if something can run on live data, rather than batched at the end,
run it on live data!

then you can get notifications of errors in real time,
and you can maybe support querying the thing you're running for its state,
and all that.
and it's faster, of course.

this is beneficial in tests of course...
but also prod...

the question is:
assuming you can run something in parallel or you can run it at the end in a batch,
and both are existing capabilities,
which one should you do?

(well actually I'm ignoring that it is actually harder in TSINT to run in parallel than in a batch,
so I actually am arguing for investing development effort)

more specifically it's like:
should you do streaming processing of data, or "at the end" in a batch?
** what is this
this is... suggestions on how to write the individual components in a single-program system!!!

how does this relate to the game idea?
well, it's pretty vague...

but I guess the game idea is specific things you should be able to do

and suggestions on writing componenets is... connecting that to real-world use cases.

but that's almost self-evident once you know the things, I think!
** sched_fifo
but with random priorities and sometimes with other schedulers too,
to ensure we don't depend on fifo scheduling.

we've never even done this though lol :(

OH!HH!H!H!H!H!H!!
OH!

oh oh!
this would be good for the game!!!!

and we're allowed to be SCHED_FIFO unprivileged if RLIMIT_RTPRIO is set

yeah this would be perfect for the game....

oh and also restricted to a single core of course
*** what's the benefit?
disables:
- memory polling
- filesystem polling
- maybe even polling other processes periodically?

well wait. it doesn't actually disable those.
you can still call "sleep" and give others time to run, hm.
it just stops busy-looping.

but, at least, it will help, I think... hmm....
*** so the real point here:
don't busy loop,
don't sleep and retry,
stop having race conditions,
don't have timeouts.

be triggered by events! stop all this other nonsense!
what's a good name for this practice? being event-driven?

what's a good name for the practice of programming where instead of doing while (!not_ready) sleep(10);
you do block_until_ready(); ?

e.g. reading from a pipe instead of repeatedly checking for a file to appear in a directory

basically, stop writing fucking shitty busy looping garbage
don't have shitty fucking timeouts

stop writing busy looping, timeout-ing, racy garbage
*** question
style 1: while (!has_data()) sleep(1); style 2: block_for_data(); style 3: call_when_data(callback);

style 1 is sometimes called "busy looping". style 2 and style 3 aren't busy looping.
what's a good single name for this "lack of busy looping" that covers both style 2 and style 3?
**** later thoughts
I guess my style 1/2/3 above is actually, very different.
1 is readiness-based, 2 and 3 are completion-based!

busy looping is also readiness-based.
*** answers
are all garbage!
everyone's saying they don't know something that covers 2 and 3, argh!

okay, I guess I have to think about this myself, SIGH

event-driven vs polling...

yielding vs busy-looping

blocking

event-driven

well. actually. isn't the busy-looping fine?

it just wastes CPU...

well it's fine if it's actually sched_yielding/sleeping; if it's just a noop it's no good.

so what are the real issues?
** real issues
you should block forever waiting for a message instead of busy-looping and then timing out.
if the other end hangs up, you'll see it by your blocking failing.

and you shouldn't be writing race conditions where you don't synchronize.

oh and also, the busy looping with a sleep slows things down! that's a big issue too.
cuz you don't get notified of the event immediately.
causing increased slowness in general.
ya it's higher-latency heh.

well it increase system-wide latency, if cores are oversubscribed.
you can *maybe* do it for shared memory communication but you should be futexing to block...
or eventfding or something to block.

yeah so:
- no busy loops/spinning/polling (which waste CPU)
  (you can avoid the in-kernel-blocking latency hit with techniques... maybe)
- no communicating via mutable state (which make it hard to detect when the other side is dead)
  (and don't support blocking, anyway. but some do I guess. like futexes... but then again, there's reliable futexes,
  which notify you if the holder of the lock is dead...
  or, no, inotify, on directories, that's a good example of what you shouldn't do)
- no sleep loops (which increase latency)
- no timeouts (which reduce stability and introduce bugs while under load) (should look into the trio docs about this)
- always synchronize (to avoid race conditions which break things)
  (rather than just sleep for an arbitrary amount of time)
  (synchronization tips... use the same path for synchronization and for operation, (like bink)
  to avoid a TOCTOU issue...)
  (and just... make sure you're actually getting state updates from the components that actually do the handling...
  you might get a state update from component A, but A just forwards requests to B, and B might not be in the same state)
** no busy looping? futex speedup?
okay so...

FUTEX_WAKE is fine, we can tolerate that kernel transition latency hit, I guess.
(worst case we can delegate the WAKE to another thread)

so we want to call WAIT if either there's no data or it's full...

ummm well.
we could spin a little in userspace before calling wake.

we could just use pthreads primitives, I guess.

ughh ummm umuhghrghu

cache line.. message passing... so bad...

well I mean, at a low-level, it's just how we implement it with these cache-based message passing.

I can't just block and wait for message

well anyway, I'll put this in with a note about,
"(except if you're dedicating cores exclusively to specific processes, and busy-looping is how cores in your CPU architecture wait for messages from other cores)"
or maybe just the part about "receiving messages", not the dedicating cores, maybe... or maybe not...
after all I don't really believe the kernel should be spinlocking either! why should it pass messages? that's stinky and gross!
(well I guess if it's messaging the scheduler core or something)
(WELL!!! no so it's... only if it knows nothing needs to run. so)

except if you're in a process with exclusive ownership of a core,
except if you're the only thing running on this core
except if nothing else is running on this core, 
except if nothing else is runnable on this core, 

except if you have exclusive ownership of the core, and busy-looping is how cores in your CPU architecture wait for messages from other cores
** concurrency title?
this is kind of tips on how to write concurrent stuff

concurrency is always relevant

unix operating system... stuff...

provides opportunities for shared state concurrency at the OS level,
without threads.

and that's bad and you should avoid screwing it up

so yeah basically it's like...
how to write concurrent Unix programs that aren't slow and unreliable

in some sense...

Systems in a Unix environment...?

no...

bah, the title doesn't matter, let's just write it.

part of the idea is...
the pain of shared memory concurrency exists even if you're single-threaded on a single machine.
because you've still got multiple concurrent processes.
and you can screw it up.
*** title thoughts
Common concurrency bugs in single-threaded systems

Tips for correct concurrency on Unix uhh

It's not really just Unix, I'm sure this all applies to Windows

Writing systems without concurrency

My tips for avoiding concurrency bugs

Concurrency bugs to avoid in distributed systems

What is it really? It's, just....

Well it's tips for writing components and a system so you don't have tons of spontaneous failures/flakes when running under unusual load (such as in tests)

Race conditions... Is a good word... Maybe...

Concurrency bugs, really...

How to write a reliable concurrent system

How to write a concurrent system that won't break horribly under unusual load

How to write a concurrent system that isn't full of latent concurrency bugs

The real world is concurrent, and you can't avoid that by avoiding being threads

Being single-threaded doesn't get you out of thinking about concurrency

Being single-threaded doesn't save you from concurrency

All systems are concurrent, here's how to write one without tons of latent concurrency bugs

All systems are concurrent; here are some common concurrency bugs

Ya that's good

Common concurrency bugs in single-threaded programs

(I'm worried that phrasing is, like, bugs caused by adding threads)

(But I think the title is compelling enough... And it's useful as a reference anyway)

(lol! I'm back to where I started)

title: Common concurrency bugs in single-threaded programs
* content
title: Common concurrency bugs in single-threaded programs

Single-threaded programs can still have concurrency bugs,
because they're almost always part of a larger multi-process concurrent system.

Here's a list of some of those bugs.
These are just the ones that I have to explain most often.
** communicating via mutable state
Some process creates or modifies a file in a directory and the other side waits for it to show up.

This gives you no indication of when the writer side, or the reader side,
are dead.
This can cause deadlocks and issues and doesn't allow you to detect failures, on either side.

Both the writer and reader need to be able detect failures!

Instead, you should use a communication channel that supports notifying you when the other side has died.
Such as pipes and sockets.

(or why is posdelta bad? well the mutable state - the directory it gets iqueues in - is just busy-looped over)
(rather than, explicitly registering iqueues)
(yeah and if we explicitly registered iqueues, we'd know if the registration failed!)

** timeouts
An arbitrary timeout on an operation is usually wrong;
you'll spontaneously fail in a different network or load environment.
Even when you might otherwise have succeeded!

Also, it makes your failure detection very slow (because you have to wait for the timeout),
which will bog down your system and cause severe issues when you actually hit failures.
(can't find stuff about this. it's kind of like bufferbloat? no trio stuff either)

Also not everything can be timed out! Sometimes you'll only receive an event once every hour or so.
You still want to be able to detect if the other side has failed!
(and heartbeats are just more epicycles)

Instead, you should... again, use a channel which supports notifying you when the other side has died.
(maybe condense multiple solutions together?)

Also, maybe... have top-level timeouts for operations?
by e.g. killing the process doing the operation if it takes too long.
or cancelling the request or whatever.

stuff on top-level timeouts:
the trio docs:
https://trio.readthedocs.io/en/stable/reference-core.html#cancellation-and-timeouts
google's RPC:
https://sre.google/sre-book/addressing-cascading-failures/#deadline-propagation-1

okay i actually grudgingly admit that some top-level timeouts are necessary,
in case of bugs causing deadlocks or infinite loops or whatever.
so maybe this section should be 100% "use top-level timeouts",
not "using a communication mechanism with failure notification"

outside of a TCP implementation, a timeout is *not a failure notification technique*.
it's a *nontermination heuristic*.
you use it because otherwise your program might never terminate,
not as a way to check if other nodes have failed.

for failure notification techniques, see the "communicating via mutable state" section above.
** you need to synchronize with other things you performed operations on; communication is not instant
And you need to actually wait for the right thing to be done

if you changed state in A, that doesn't mean that changed state is instantly reflected in B.
you aren't guaranteed to be able to talk to B right away.
OBVIOUSLY.

(and just... make sure you're actually getting state updates from the components that actually do the handling...
you might get a state update from component A, but A just forwards requests to B, and B might not be in the same state)

this is actually separate from the sleeping thing actually

what's this even about?

this is definitely the most subtle point here.

so it's kind of like TOCTOU issues but...

the issue is when we're not even checking the right thing.

well!! actually!!!

normally we wouldn't be able to just "check" something to see if it works.

because we're in a live system.
we can't just check, even if we're checking the right thing,
because it might change again...

so actually.... maybe this "wait for X to happen" approach is bad anyway...

maybe I should just... try... and loop?
but no, locking would be better...

yeah, the trick is that in a non-test, this is something we'd witness as a TOCTOU issue.

because we can't merely check (even if we check the right place) and assume things are working.
it can always fail! due to TOCTOUs!!

hmmmmmm

well what if we...
try, then wait for the failure condition to clear,
then try again, then wait again, etc?

hmmmmmMMMMM

it's like, I try and take the lock, I fail,
I wait to take it,
when I'm woken up, I try and take it,
and then I fail again...

kind of... like a... condvar or things like that..........

but still, okay, that's not really a clean way... since it's vulnerable to thundering herds...

again, so what does this original problem correspond to, in a real system?
should I really just be polling, trying repeatedly until I fail or succeed?

no wait, it's kind of like a... readiness notification.

where you can wake up and try to read and get nothing and have to wait again

so it's like... watch for the right readiness notification! or you'll suffer a deadlock!

yeah so maybe I should adjust my other ones, becuase...
busy looping is bad, and sleep looping is bad,
but looping on a "wait for readiness, try operation, back to start if failure"
is fine...

well, it's risking thundering herds, but it's mostly fine...

well how do we avoid thundering herds?
we wake up only one thing...

ala EPOLLEXCLUSIVE or EPOLLONESHOT or EPOLLET as such..

but I can see, completion notification is a little nicer... hmm...

so, I mean, if we have multiple people using some resource,
we can't just say "hey you, you get the notification that the resource is available",
because that's tantamount to just giving them the resource.
I mean, that's basically a completion API!

well I mean, I guess it does give the one who was woken up the resource...

and if they crash that's bad

hmm, in a real world tactic case,
I really would sit there waiting for marketdata to be good,
send an order when it is,
stop sending when I get a reject for bad marketdata,
and then wait once more for marketdata to be good.

i guess this is like waiting for a process to start up?
except it can go down and then you can need to wait for it to come back up

hard to summarize/explain for normies though...

since I think they usually have some kind of...
well, manual failover,
but also load balancers to automatically send to a working service...

but what if things are down? I guess they just do manual failover...

or the connection drops and they try to reconnect, I guess.
but no, we can also have individual instruments/lines (services) go down.
we can't just drop the connection because of that!

okay so this is a followup to "sleeping is not a notification mechanism".

that we need to use the *right* notification mechanism.

the solution to sleeping is not "block until you're notified".
it's "have a notification exist at all".
the blocking isn't the solution, the notification is.

because we might still need to loop on the notification!

RIGHT!
don't just sleep and retry!
have an actual notification about whether something is ready or not!!!!!
this is about retries!!! yes!!!
have an actual notification system about readiness! PERFECT!

so it's:

- don't just loop and sleep on your retries, have an actual notification mechanism.
and,
- make sure you're actually getting notifications from the right place.

oh, also:
- do retry! retry! don't just fail the first time...? maybe?

so I actually don't dislike loops!
er maybe...
*** retries
retries, do I like them or not?

well it's a question of whether you crash or not on failure.

crash on failure - which is very often a good idea -
or retry indefinitely - until you get timed out or some other kind of more severe failure happens.

prefer to crash on failure.
but, if you can't (which is rare)...

right, crashing on failure puts the retry logic one level up.
you crash, and someone above you has to decide whether to retry or not.
(like the inherited deadlines thing)

but, if you are the one implementing retrying, do some good things

that is, if you are the one handling failures, do some good things.

well yeah... it's fine to retry transient errors.
if it's not transient, then crash! simple as.

even if it's transient... if you have no guarantee that it will ever be fixed... just crash.

mmm... hm... hmmMMm..

right, if you don't know if it will be fixed, then... just... crash...........
rather than implementing retry logic.......

can I work this in with the tests?
why in tests should you not retry?
well because... it probably will never be fixed? maybe...????

in a test we're basically single-threaded:
we're the one who would fix it, and we're the one waiting for it.
so we can't just retry. because... we're the one who would fix it!

that doesn't really apply for a service crashing on failure to talk to an upstream system, hm....

well, okay, so.

I guess this also is, make sure your upstream service is ready for requests before you send requests.
have a notification mechanism.

- make sure a service is ready for requests before you send it requests.
- don't just do that by sleeping (or by running at a specific wallclock time which is after the upstream has started, which is equivalent to a sleep.
  e.g. don't have two cron jobs, one at 8:55, the next at 9:00, where the second blindly expects the first to be done when it runs),
  instead, have a notification mechanism for when upstream is ready, so you're sure.
- make sure that notification mechanism is actually for the right place. and of course make sure that you can block on it so you know if the service will never be ready.

and then. if you want to retry, you can wrap this all in a retry,
but most of the time you shouldn't retry.

and we can s/ready for requests/in the appropriate state for your requests, before you send requests/
because sometimes we want to fail or something, in a test.
we can just clarify: "ready" means "in the appropriate state for your requests", which might depend on your application.
(e.g. maybe you want the service to be down for maintenance!)
*** readiness notification
there's a surprising amount of robust readiness notification in TS systems
it's weird...
do other systems do this?? I've never really heard of it...
*** trading example of the right notification
getting notification of LULD resumption from MD and then sending orders,
rather than waiting for the resumption notification from the order entry system.
** sleeping is not a synchronization (notification?) mechanism
After starting an operation,
don't just sleep to wait for it to be done.
You have to wait for the operation in a way specific to the operation.

And these increase latency, anyway!
Because it might finish before your sleep is through.
Which makes your system slow, in addition to buggy.

try blocking on a read syscall on a pipe or socket.
** no busy loops
I don't like busy loops wrapped around sleeps checking some mutable state...
but........
why?

hmmmmmm I can't think of any small improvement to make it more palatable,
fixing any of these things would fix all

okay so wrapping a loop around the sleep does fix the "possibly buggy" part.
it doesn't fix the potential deadlock, but slapping a timeout on fixes that.

lol so:
timeout on a busy loop around a sleep, which is repeatedly checking some mutable state.

the worst! garbage! trash!

okay actually so the mutable state deadlock problem we can avoid by checking an NONBLOCK pipe...
and we can remove the sleep just for fun...

then it's just busy looping on a NONBLOCK pipe.
okay, so what's wrong with that?

well, obviously the CPU usage...
ummm and it won't work on SCHED_FIFO I guess
*** this is really about retries
and I can just omit it because actually some retries are fine,
as long as we're not doing it by sleeping,
that is, as long we're actually waiting until we have a reason to retry.
* content, attempt 2
** wait for a service to be ready before you send it requests
Ummm well...
I don't want to suggest that individual services should do this...

Preferably outside...

Maybe I just mean, track...

Okay let's just say it as...

have a notification mechanism for when the service is ready.

one which clients should use, I guess?

right basically just.
have a notification mechanism for when your state changes in ways that are relevant to your client.

that is, when things will start failing or succeeding or things like that.

*** notify your clients when operations will start failing (or succeeding)
and your clients should wait for that

no this should be focused on the client

title: wait until your operations will succeed before trying them
* content, attempt 3
** wait until your operations will succeed before trying them
very frequently, an operation will fail unless X other side is in the right state.
somewhere, somehow, you should wait for X to be in the right state before doing the operation.
don't just blindly do the operation!
you might get lucky, maybe even 99.9% of the time,
but under load or scheduling changes,
you'll eventually get unlucky and cause breakages!

be sure to distinguish this from operations which themselves don't return until they've succeeded.
a blocking read, for example;
you need to wait until the pipe exists at all,
but once it does, you can perform a blocking read without having to wait for it to be ready.
you should prefer this wherever possible, but it's often not possible.

Having failure detection in the waiting process is important here too.
You should know when the thing you're waiting for will never happen,
and you should do that by using communication mechanisms with built-in failure detection.

achieving good support for such waiting will often require a high-quality service implementation
which exposes such notifications/waiting support,
which is sadly rare,
so don't be afraid of going in and adding support for these kind of status updates.

Things this covers:
- starting up servers and waiting for them to be ready;
  you can often avoid needing to wait in this scenario by using socket activation
- waiting for O_NONBLOCK fds to be readable
- state changes in processes
  (what's a good normal-person example? I guess I can just say "a certain stock being no longer available for trading")

*** retry thinking
so what's the intent with the retries?
well...

basically that you shouldn't need retries if you're doing the rest right

retries should happen at a higher level,
or better yet, shouldn't be necessary at all.

you should arrange things so that you don't need to retry in a loop.
** don't just sleep to implement this waiting
Just throwing a sleep(5) into your program instead of waiting for explicit notifications
is a good way to make a program which is slow (waiting too long) and buggy (not waiting long enough).

Don't ever sleep.
** don't wait for the wrong thing
Sometimes you can be performing operations on one thing,
and the status updates you're getting are for a different thing,
but it's easy to confuse them.

Remember that communication is never instant;
just because service X is ready, or has seen some event, or something,
doesn't mean that service Y has.

And remember that some services forward operations to other services;
they may be incorrectly coded.

Make sure that when they're saying "X is ready",
that means any requests which come *instantaneously* at that point will succeed.
X shouldn't say "X is ready" until that's true.

It's always better to delay a ready-notification,
than to issue it too soon.
If you issue it too soon, the ready-notification is useless to your client:
No matter how many times they wait for ready-notifications,
they're never guaranteed to get an operation that passes,
if they send operations fast enough.
** don't use timeouts
Don't use timeouts.

Remember,
outside of a TCP implementation, a timeout is *not a failure notification technique*.
It's a *nontermination heuristic*.
You use it because otherwise your program might never terminate,
not as a way to check if other nodes have failed.

How do you see if other nodes have failed?
See the "communicating via mutable state" section above.

An arbitrary timeout on an operation is usually wrong;
you'll spontaneously fail in a different network or load environment.
Even when you might otherwise have succeeded!

Also, it makes your failure detection very slow (because you have to wait for the timeout),
which will bog down your system and cause severe issues when you actually hit failures.
(can't find stuff about this. it's kind of like bufferbloat? no trio stuff either)

Timeouts should only exist at the very top-level - the user interface.
So these days, if you're not writing Javascript that runs in a web browser,
you probably shouldn't be writing timeouts - and often not even then.

stuff on top-level timeouts:
the trio docs:
https://trio.readthedocs.io/en/stable/reference-core.html#cancellation-and-timeouts
google's RPC:
https://sre.google/sre-book/addressing-cascading-failures/#deadline-propagation-1

so this section, I should retitle:
stick to top-level timeouts
or:
don't add more timeouts
or:
stick to a single timeout

it should be clear for tsint programmers...

don't add more timeouts is, I think, the right way.

if there's no top-level timeout, don't add any.
if there's one, don't add more.

anyway so, title:
don't add more timeouts

or... maybe...???
"use timeouts to check for nontermination, not a means of failure detection" 

well that's what people are doing, actually, checking for nontermination (??? or are they?).
it's just that, it's pointless for them to do that, because there's already a global timeout.

let's focus on that (not duplicating the global timeout) I guess.

wait I don't even believe in the global timeout
things should just be interruptible/cancellable all the way down.

what even is cancellation... it's like...
well hey, how would I handle cancellation in dneio?
oh that's right, I don't.
you can't cancel things, you can just stop waiting for them.

yeah so.. hm...
don't add timeouts...
don't even be cancellable?

except... if we take too long we'll begin to suspect it's broken and we want to free the resources.
so... maybe the global timeout is fine.
that's a usecase which isn't UI-driven.
but it's what happens if we're doing multiple things at once...
*** don't add timeouts
At the UI level, or some other top-level, there are timeouts.
Anywhere else, you shouldn't be adding timeouts.

Remember,
outside of a TCP implementation, a timeout is *not a failure detection technique*.
If you want that, use communication mechanisms which have built-in failure detection, as mentioned previously.

A timeout is a *nontermination checker*.
You use it because otherwise your program might never terminate,
not as a way to check if other nodes have failed.

And you only need one of those, at the top-level,
to make sure that your program does in fact terminate.

An arbitrary timeout on an operation is usually wrong;
you'll spontaneously fail in a different network or load environment.
Even when you might otherwise have succeeded!

Also, it makes your failure detection very slow (because you have to wait for the timeout),
which will bog down your system and cause severe issues when you actually hit failures.
(can't find stuff about this. it's kind of like bufferbloat? no trio stuff either)

Timeouts should only exist at the very top-level - the user interface.
So these days, if you're not writing Javascript that runs in a web browser,
you probably shouldn't be writing timeouts - and often not even then.

stuff on top-level timeouts:
the trio docs:
https://trio.readthedocs.io/en/stable/reference-core.html#cancellation-and-timeouts
google's RPC:
https://sre.google/sre-book/addressing-cascading-failures/#deadline-propagation-1

** don't communicate via mutable state
Some process creates or modifies a file in a directory and the other side waits for it to show up.

This gives you no indication of when the writer side, or the reader side,
are dead.
This can cause deadlocks and issues and doesn't allow you to detect failures, on either side.

Both the writer and reader need to be able detect failures!

Instead, you should use a communication channel that supports notifying you when the other side has died.
Such as pipes and sockets.

(or why is posdelta bad? well the mutable state - the directory it gets iqueues in - is just busy-looped over)
(rather than, explicitly registering iqueues)
(yeah and if we explicitly registered iqueues, we'd know if the registration failed!)
*** retitle?
maybe I should retitle this section to more focus on,
"use communication mechanisms which have built-in failure detection".

and I can just say, "hey mutable state doesn't provide that, lol!"

mutable state... also... well, shrug, blah.
it has... race condition possibilities...
but even without that. doesn't really matter!

mlist, e.g., doesn't have race conditions!
but it still doesn't have failure detection.

yeah I think this is a good retitle.
*** use communication mechanisms which have built-in failure detection
You should use a communication channel that supports notifying you when the other side has died.
Such as pipes and sockets.

Rather than, for example, "writer creates or modifies a file in a directory, reader waits for it to show up".
This gives you no indication of when the writer side, or the reader side,
are dead.
This can cause deadlocks and issues and doesn't allow you to detect failures, on either side.
* Tips for concurrent programming
Here are some tips for concurrent programming.
** Use communication mechanisms which have built-in failure detection
You should use a communication mechanism that supports notifying you when the other side has failed.
Such as pipes and sockets,
which will return an EPIPE or other error if you read or write when there's no-one on the other side.

Not having a built-in failure detection system can cause deadlocks and issues, on both the sender and receiver side.

Consider, for example, "writer creates a file in a directory, reader waits for it to show up".
This gives the writer no indication if the reader is dead and will never receive the file,
and the reader no indication if the writer is dead and will never send the file.

Here are a few examples of communication mechanisms with failure detection:
- Pipes
- Socketpairs
- Unix domain sockets
- TCP connection sockets
** don't add timeouts
An arbitrary timeout on an operation is usually wrong;
you'll spontaneously fail in a different network or load environment,
even when you might otherwise have succeeded.

A timeout is a way to avoid nontermination, not a way to detect failures.
You should detect failures by using communication mechanisms with built-in failure detection,
not with timeouts.
Timeout-based failure detection will increase the latency of your system and decrease its ability to respond to failures,
because you have to wait for the timeout to know a failure has happened.

The right reason to use a timeout is because otherwise your program might never terminate.
And you only need one such timeout, at the top-level or in the user interface,
to make sure that your program does in fact terminate.
So don't add more timeouts on your own;
whoever is running your program or calling your service is the one responsible for timing out.

Some docs on this:
- Google calls it "deadline propagation"
  https://sre.google/sre-book/addressing-cascading-failures/#deadline-propagation-1
- The trio concurrency library docs talk about it:
  https://trio.readthedocs.io/en/stable/reference-core.html#cancellation-and-timeouts
** prefer completion interfaces to readiness interfaces
Prefer interfaces where you send a request and don't get a response until the operation is complete.
This is sometimes called "completion notification", or "the completion model for async",
and it's easier to use and easier to get right.
See discussions of [[http://lkml.iu.edu/hypermail/linux/kernel/0010.3/0849.html][readiness vs completion models]].

For example, suppose you're trying to allocate some resource,
and there's no resources available when you send the request.
In a completion interface, you just send the request,
and wait however long (minutes, hours) that it takes for the response to come back with an allocated resource.

The alternative is a readiness interface;
where you wait for the right state for the operation,
and only then do it.
For example, you wait until you receive a notification telling you that there are resources available,
and then you send a separate request to allocate that resource.

A readiness interface requires the implementer to spend less resources on tracking outstanding requests,
but it's harder for the user to use correctly.
In particular, in a completion interface,
waiting and operating are coupled together,
so it's impossible to wait for the "wrong state" for a given operation,
or wait for the state in an incorrect way.
In readiness interfaces,
it's all too easy to wait on the wrong thing,
as I'll discuss in the next section.

Some completion interfaces:
- the "read" and "write" syscalls
- using socket activation when starting up processes so clients can immediately send requests
- IOCP on Windows
7** prefer readiness interfaces to sleeping (or nothing)
In a readiness interface, you wait for the right state for an operation before performing it.

A surprisingly common misdesign is to initiate some preparations,
then to simply sleep for some number of seconds,
and assume that everything is in the right state once you wake up.
This will resulting in a program which is both slow,
because it waits longer than it needs to,
and buggy,
because sometimes it doesn't wait long enough and causes failures.

Another surprisingly common misdesign is to not sleep or wait at all,
but to initiate some preparations and then to just assume that everything is ready immediately afterwards.
This is like sleeping,
but for a random amount of time which depends on how long your code takes to run.
If you actually instantaneously performed the operation after initiating the request,
you'd always fail,
but because there's some small delay,
you might get lucky 99.99% of the time.
Eventually, however, under load or with bad scheduling,
you'll get unlucky and things will break.

Instead, you should wait for things to be in the correct state.
You'll need to receive some kind of notification
about the state of various entities involved in whatever operation you're performing.
If you're writing client-side code,
don't be afraid of adding support for these kinds of status updates on the server-side.
Again, make sure you use communication mechanisms with built-in failure detection.

Some readiness interfaces:
- the server can only perform certain requests when they're allowed;
  the client has to wait for the server to notify that the requests are allowed
  to be able to send those requests without encountering failures
- starting up daemons and waiting for them to be ready
- waiting for O_NONBLOCK fds to be readable

Some more tips on readiness interfaces:
*** when using readiness interfaces, don't wait for the wrong thing
Sometimes you can be performing operations on one thing,
and the status updates you're getting are for a different thing,
and it's easy to confuse them.

Remember that communication is never instant;
just because service X is ready, or has seen some event, or something,
doesn't mean that service Y has.

And remember that some services forward operations to other services;
you need status updates based on the state of the service you're actually interacting with,
not just the proxy.
*** when implementing readiness interfaces, don't send ready-notifications too soon
Make sure that when you say "X is ready",
that means any requests for X which come *instantaneously* at that point will succeed.
Don't say "X is ready" until that's true.

It's always better to delay a ready-notification,
than to issue it too soon.
If you issue it too soon, the ready-notification is useless to your client:
They can't send an operation immediately, because it might still fail.
They have to resort to sleeping - exactly what we were trying to avoid.
** don't use retries
A combination of all the above techniques will usually save you from needing to retry.

Retries:
- mask serious failures, rather than letting them be appropriately handled.
- waste CPU time

Even if you might want to retry,
you can often just fail instead,
and let the next level up above you deal with retrying.
Just let the exception propagate and let your caller handle it;
just crash your process and let your supervisor restart it.

But sometimes, retrying is part of a correct algorithm.
For example,
a readiness interface can require retrying due to TOCTOU issues,
and a completion interface without [[https://en.wikipedia.org/wiki/PCLSRing][PCLSRing]] can need to be retried on the client side.

Just make sure that, before you resort to retries, you've fully implemented all the other techniques in this article.

Some examples of necessary retrying:
- the server said it was ready, and the client sent a request,
  but the server became non-ready before receiving the request and returned a failure to the client,
  so the client needs to wait for it to be ready again
- an FD no longer readable after you're woken up, because some other thread consumed the event
- condition variable usage
- spinlock implementation
- calling read again after a partial read returning less data than you want
** prefer failing up to retrying
prefer retrying to failing silently and just sitting there in an error state

yes, okay, this is another good ordering.

should say, prefer propagating errors to retrying

and prefer retrying to silently doing nothing

(hah what does this mean in the context of people...)

anyway.
prefer to fail up instead of retrying

do I really prefer retrying to sitting there and doing nothing?

also, sometimes failing up isn't actually the best thing to do.

what are these different scenarios, hmm.

like, sometimes I prefer not retrying, and instead failing up
and other times, I don't.

yes, I should inspect this.
so... if I have a completion interface that can transiently fail...
that is, I believe it should work again later...

then I should retry...

but then again - I guess if I retry anyway and it fails again immediately,
that's not really completion.

so it's more like, I should fail up if it would fail immediately when I wait next time.

(also, I should fail up if I have reason to believe it will never work again...)
like in the marketdata scenario, where I'm controlling MD.

okay, so, of course I should fail up if I can't wait anymore.
and... I guess... I should fail up if I know waiting again would not terminate...

but then again, isn't that something a timeout should detect?

hm.


well, more early failures is better...

also.. hm.. timeouts...

like, if I keep getting some specific failure and keep retrying it...

how do I know that it won't be fixed by someone?
well, if I'm the one fixing things.
but that's not something most applications can claim...

okaaay....

again, early failures are nice...

okay so if i'm calling into some thing,
and it fails because X,
that tells me that X is broken and I need to fix it.

I don't necessarily have any other way to see that;
I mean, I can have monitoring,
but the only robust way to know if something works,
is to know if the things that use it are working.
really, that's the definition of it working!
(this is much like usertests...)

on the other hand, state changes aren't "failures" as such.
like, they're within the range of normal operation.

because it's an open system.

state changes in a test though... then they are not within the range of normal operation.

and also, services going down aren't within the range of normal operation.

i mean... if I'm getting a failure, the question is:
will just waiting fix it?
or does someone need to do something somewhere?

well... in those words, clearly any kind of MD failure or halt or something,
needs an action from someone...

and if a resource is exhausted and I'm waiting for more,
someone needs to return the resource...
(or stop spinning and taking it repeatedly...)


hm.

so maybe if I'm some tactic sending orders in one instrument,
and I get a halt in that instrument,
I should indeed just crash.

then whoever is scheduling each individual instrument-tactic,
can restart me, after waiting for that instrument to be ready...

likewise if I can't get resources, I guess...???

okay so what would that mean, if I crashed after getting halted?
then someone above me would restart me after the instrument is unhalted.
they do the waiting in a retry loop.

but that basically means they're also in a retry loop.
so how do they know that the instrument is going to be unhalted?

they want to say...
"hey! someone! unhalt the instrument, please!"

so maybe that's what you need to do in a retry loop?
in plebian terms, log;
in patrician terms, wish.

so okay, that's the best they can do...
and they don't want to crash everything else...

they just want to signal someone to fix it...

and, of course, if there *is no signal* that they can send to get someone to fix it,
then the signal they should send is to crash
(that's a test's situation)

yeah, okay, so, the question is,
why does an instrument-tactic crash,
and the broader thing does not?

well, if we can't function at all while we're in a bad state,
then we should crash.
maybe that's the guideline?

this justifies bump crashing on posdelta timeouts, for example

we should crash, because that's the best way to signal that things are broken.

if we can still do other useful, productive things
then we should stay up.
(and we should have a high standard for those useful productive things,
because crashing is so nice),

okay, so how does this fit in with spinlocks and partial reads?
why should someone above me retry when there's not enough data?

well actually...
if we're requested to get so-and-so amount of data...

we maybe should signal "hey we want more data".
and actually, we are signaling that! by removing backpressure.

because when there's backpressure, more data can't be produced.
so without backpressure, more data can be produced
(and we assume in a steady state it will be, I guess, so we don't have to signal)

with a spinlock...
we want to signal, "hey, we want the lock, please release it".

i guess it's like...

if you're not reading then backpressure will happen on the writer
so if you are reading, then the writer can write

if you are spinning on the lock... how does that encourage releasing the lock?

if no-one is spinning, then...
well, someone could hold the lock forever...?
or...
it means no-one needs the lock? so it's not an active lock,
it's not contended?

if a lock is contended...
I guess it's kind of like a prisoner's dilemma heh;
if you give up the lock faster,
you'll get the lock faster!

yeah, that's a really interesting observation!

a selfish thread could hold the lock forever,
defecting against others to get the wonderful value of exlusive access to the datastructure.

but a cooperating thread will release the lock, cooperating,
because this is an iterated PD,
and it will get the benefit of future cooperates:
future fast releases of the lock by others...

when the lock is uncontended, you can hold it forever, whatever...

it's contention that sets up the PD incentive to release quickly.
*** message to kai
why should any thread give up the lock quickly?

because it expects to want to take the lock again later,
and when the spinlock is contended, it will get it faster if others give up the lock quickly

maybe the thread might want to hold the lock for longer so it can do more work
but that would be a defection!

if it instead cooperates, as part of a cooperate/cooperate strategy
then it will be better off, since others will also cooperate
in this way, selfish threads can sustain an equilibrium of short spinlock hold times

of course, it's more of a continuous PD, which makes it more complicated...
it's hard to judge whether someone defected,
or if they just got scheduled poorly or something and had to hold the lock for longer
classic PD issue
iterated prisoner's dilemma with noise, it's known as

so the continuous nature isn't the important part,
it's the fact that there are different use cases which need different amount of usage,
and they can have noise introduced by the scheduler
*** hah
I love this game theoretic analysis of spinlocks!

so yeah. there's, like,
an equilibrium strategy, game theoretic reason
why you should give up the lock quickly.

although I guess this is only when it's contended - when it's not,
maybe you could hold it for longer...

but since you don't have any notification of when someone else is spinning,
you have to cooperate all the time.

I guess this gives some insight into PD too...

like what if it's prisoner's dilemma where sometimes there isn't another prisoner?

prisoner's dilemma where sometimes randomly there isn't another prisoner!

that's an interesting scenario!
**** more thoughts
incompetent programming is also a possible defense
"I don't know how to give up the spinlock fast, I write slow code because I'm incompetent!"
"so don't punish me, please!"
*** okay so what does this mean
basically...

you can retry when you've sent a signal to others that they should "fix things"
(such as by giving up the lock)

spinning on a spinlock is a *counterfactual signal*.
they don't have to actually receive it,
to know that they need to give up the lock quickly.

(although... maybe the chance of there not being another thread/prisoner...
gives you some leeway to probabilisitically defect,
which is maybe equivalent to,
a certain (not probabilisitic) greater amount of work under the lock)

very interesting.

(i've been reading EY's new fiction too much, lol, that's why I'm thinking this way)
*** okay
okay, so.

- prefer crashing to logging (wishing, notifying). that's a core point.
  (if the error doesn't stop you from performing other functions,
  then you can log, but you should prefer to crash)
  (i.e. crash/raise until you reach the level
  where the error doesn't stop you from performing other functions)
  (so, maybe:
  fail/crash/throw upwards, until you reach a point
  where the error stops preventing you from doing work;
  then and there, try to fix it.
  um... anyway that should be the headline)
- only retry if you've done something to fix the issue
  (such as wishing)
  (or possibly just logging/signaling it somehow)
  (possibly counterfactually... although it should be a strong counterfactual equilibrium)
  (yeah, "done something to fix the issue" includes
  "counterfactually cooperating to create a C/C equilibrium")

  

wait okay crashing is tied to retrying.

basically, crash unless you've done something to fix the issue
don't just continue on in a state where you can't do anything.

ummm..
well, okay, what if I haven't done anything to fix the issue,
but I havem more responsibilities to do?

or what if I've done something to fix the issue,
but I don't have any more responsibilities?

okay, then it's clear;
we shouldn't be doing things to fix the issue unless we need to keep running.

so, prefer to crash until the error isn't stopping you
from fulfilling other responsibilities.
(doing at least a little work?)
prefer to crash until you reach the point
where the error isn't stopping you from doing at least a little work

(like read return partial reads)

then, there, try to fix it.

um, so...

does this advice fit tests?

well in a test...
if I see an error...

I can't really fix it,
because it implies a logic error!

that's down to the programmer!
I signal to them, by failing!

In a supervisor, we can have logic errors...
well, all things are logic errors...

no no, in a test, the only possible issue is a logic error.
whereas in prod, there are real other issues that can happen too.

the fact that we can tolerate them *is our logic* that we've written

if we see an error in a test method, on the other hand,
the error indicates that our logic, including any error-toleration, *has failed*

raise it back to the programmer so they can figure out why...
(but the answer isn't to just add a retry.
so... hm.)

well actually if we were trying to break a system and then test the repair logic...
it would be right to fix the repair logic.

but not add retries to the test itself.

mmm mmm hmmmmm sure? maybe?

hmmmmm

okay so sure. only add retries at the level that already handles multiple things,
is very plausible.

oh, and the test method doesn't handle multiple things, I guess?
can we make that argument?

well we can certainly say, like,
"A test, for example, handles one thing:
performing a sequence of events and assertions to validate a system.
Retrying the test, or anything inside it, should be handle at a higher level:
The test runner, whether that's an automated system or a human."

sure, that sounds plausible.


okay, so next question:
why don't we just fail out of a spinlock?
it only handles one thing.
so what's the big idea, retrying, eh?

well uhhhhhhhhhhhhhhhhhhhhhhhhhh

uhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh

I guess all it handles is retrying...?????
it's a completion API?

but so is "read_N_bytes"....

hmmmmmmmmmm

well in the bump case....
the failure is in something we... need to continue.
so we... fail

hmmmm

also, I'd probably want to just wrap a retry loop around the instrument-tactic anyway.

like I wouldn't have any fancy logic there...

and that's a function that does one thing!

so when can I fail and when can I not fail?
I don't think "fail up to multiple responsibilities" is right!

since spinlocks don't fail and I think that (has to be) fine...

I guess...
maybe I fail up to the point where I can do something about it, then?
like, bump can't fix posdelta...

but a spinlock contention can't unlock the lock...
but it can counterfactually...

posdelta doesn't rely on bump as such; well... it does in some sense...

okay, so fine, I fail up to the point where I can do something about it.

or...

I fail up to the first point where I can either:
- do something about it, or
- I have multiple responsbilities.

if I have multiple responsibilities then I guess I have to do something about it.

so I guess it's more:

fail up to the first point where you can do something useful about the problem!

how do we get this as pithy as "prefer failing over logging an error and continuing"?

maybe that can be the title,
and we can just say:
logging an error and continuing (or, "trying to fix the situation"?)
is appropriate in some situations;
like when you have multiple responsibilities,
or when you can actually fix the situation.

but try as much as possible to just fail instead.

recovering is the right word!

prefer throwing/raising/aborting/crashing over recovering

recovering is appropriate when you really can recover.

maybe I should say, also,

prefer recovering over logging and continuing/retrying

ummm...
hmmm......

well, recovering is not always better, actually.
i can imagine it being a nightmare.
and logging *is* a way to recover....

prefer throwing/raising/aborting/crashing over doing something not useful
(like, we want to fail until we can get to someone who can really fix the problem)
(but also, if we're doing other things... we need to stop...)

prefer throwing/raising/aborting/crashing over being in a bad state where you're useless

like, the bad situation is... if you're just spinning trying to reconnect, say

urgh maybe also I should factor in something about not having useless error logs;
like if you log an error, it should be stopping you from doing something!
like... you should fail, maybe, instead!

lol, well,
"prefer throwing/raising/aborting/crashing over doing something not useful"
is a useless thing to say.

obviously doing something is better than doing something not useful.
since there's a chance that something is useful...

actually...
"prefer to crash rather than do nothing"
that's actually... a pretty incisive comment.

that's basically,
"prefer to crash until you reach the point,
where the error isn't stopping you from doing at least a little work"

and also, rather than hanging around in a bad state, you should just crash.

now, I guess you could log errors,
and assume someone will get them.

so it's:

crash > log errors > do nothing

we're back at this again.

crash > recover and retry > do nothing

crashing punts it up a level,
which is something you should always want to do.

on the other hand,
if there error isn't preventing you from completing all your responsibilities,
you can't viably crash.
in which case, proceed to the next section:

recovering and retrying is better than doing nothing and retrying.
don't retry unless you have some reason to believe things will be fixed!
(counterfactual spinlock...)

if you have no reason to believe things will be fixed,
but you have other responsibilities...
ugh...
sigh....

crashing is the best way to notify your parent...

well, okay, so actually,
doing something to fix the problem when you don't have other responsibilities is valid, too.

oh! right!

design your systems such that for any problem, they can either:
- fix the problem
- they can reasonably just crash/fail (and make it someone else's problem)

right!
we should make this situation of,
I have many responsibilities but I can't fail,
impossible!

most systems should not be able to fix most problems,
so you should divide responsibilities,
such that they can just crash until they hit someone who can

so, title:

either fix the problem correctly, or just crash/fail.

don't limp along in a degraded state;
either fix the problem,
or crash!

responsibilities should be divided so that this is always possible.

fixing the problem may involve signaling someone...
well... okay... logging is a pretty bad way of fixing the problem.

oh, also, this doesn't actually cover what to do when, e.g.,
a single instrument goes down.

ARGH, COOL, COOL, MORE THINKING
sure do love to endlessly think in circles.

OK!
I like the idea of "either fix the problem or crash".

but how does this fit in with multi-responsibility systems,
which can't fix certain problems which don't interfere with all their responsibilities?

right, that's a real situation!

then, well, perhaps it's reasonable to just log,
or to just do something to fix the problem...

okay, well, maybe we can say: those systems should be able to fix the problem!

e.g. bump, when an instrument is halted,
should... appropriately scream?

um. blugh??
what does it do today? nothing, basically.
it just starts rejecting orders.

and it marks the instrument as down.

but... this isn't really a *problem* for bump.
like, it's just a normal state.

marketdata being down, now that's a problem...
and it can scream about that, I guess?

hmMMMMmmmMmmMMmMMMmmMMmm

it really should be able to just...
fail? really tho?
i mean, sure, why not!
it's not all that transient!
and it fully prevents running!

rather than mark in the validators that it's bad or whatever, sigh.

i mean, yeah! I actually think that would probably be good.
it would bring down the tactic etc,
but we'd restart everything when bump came back up (which would happen when MD comes up)

(we'd need to recover, but let's ignore that cost;
maybe we'd have some clean way to minimize the cost of recovering state)

heck we could even have HFMD go down when MD goes down, cascading failure all the way.

meanwhile if we can keep running, I guess,
we do?

e.g. like the disemminator multiplexing over the SIP feeds

okaaay soooo.
multiplexing, is that a good word here?

no not really.
I mean, it's more like,
we're a channel selector or something.

anyway.

so if we have multiple things we can do and one goes down,
what do we do?
what do... WE DO...????

i mean obviously we try to fix it.

so like...
there should be some thing that is responsible for fixing that thing,
and it should be... us....???

wait, but the tactic will also see/hear that the thing is down.

so maybe... that's an appropriate propagation up?

right, we can return an error status for that instrument,
to punt the problem up to the tactic.

okay, so that, actually seems reasonably good?

like, if you can't fix the problem,
at least push it upstream.

*rather than* just logging blindly.

like, tell your upstream about failures, basically.
whether that's by crashing, by throwing, or by sending status updates.

okay.
so.

either push failures upstream, or fix the problem yourself.

don't just log a failure and continue on in a bad state.

abort! throw! send "IN_BAD_STATE" status updates to your clients!

aborting/throwing is better if you aren't multiplexing.
pushing it upstream is your only non-fix-it option if you are multiplexing.

(crap I don't like the word multiplexing here. but. ugh, it's stuck now)

arbitrating?
multiplexing is kind of right, sigh.

it's not just making multiple out of one,
it's making one out of multiple.

combining
i'll just say combining.

so yeah, push failures upstream.
notifying your clients is your only non-fix-it option if you're combining.
(maybe multiplexing works actually)

okay. so.

yeah. this seems right.

and retrying is a way to fix the problem yourself...
but you shouldn't retry if you can push the failure upstream.

well...
no, you can retry, but only if you can *actually fix the problem*.

can you fix this failing server when you retry in the client?

either push failures upstream, or fix the problem yourself.

don't just log a failure and continue on in a bad state,
or retry blindly,
or other such things.

you can retry, but only if you're *actually fixing the problem*
in the server before you retry.

okay, so what does "actually fixing the problem" mean?
well, it means...
if you're doing the best thing that's possible globally?

like, if your response is worse than what someone else could do,
you should push the failure upstream,
and let them respond to the failure.

that's actually a really good rule/phrasing.

don't do suboptimal retrying or fixes yourself,
push the problem upstream so someone at a higher level can deal with it;
the highest level is the human operator,
but most problems can be fixed before that.

only do a fix yourself if it's actually the globally optimal fix.

okay, I like this, I like this.

this covers:
- spinlocks (spinning is optimal)
- read (push partial reads up)
- read_exeact (retrying is optimal)
- bump instruments going down (push it to the tactic)
- bump seeing posdelta/marketdata going down (crash, push it to the supervisor)
- an FD is no longer readable after wakeup (waiting again is uptimal)

what about...
retrying after a state change

"if the state is changed by events, retrying is optimal"?

is that true?
so we're thinking like, bump sends us "you're all good to send orders!"
then we send, and get rejected.
do we just wait again?
sure, waiting again is optimal - either we'll get more information with a failure,
or it was a transient error.

yeah, and the globally optimal thing to do with transient errors is to retry.

but the globally optimal thing to do with permanent errors is to restart the service,
or something,
which a random client can't do,
so it needs to push the failure upstream.

okay.
so how do we word this?

one phraseing:

prefer to push errors upstream than to do a non-globally-optimal fix

except that's highly obscure what I'm even saying.

one manifestation of that is:
prefer to abort than to log an error and continue

(unless the only thing someone could do globally is to just continue)

another phrasing:

push errors upstream if you can't fix the problem locally

push errors upstream if you can't *properly* fix the problem locally?

no, no adjectives.

push errors upstream if you can't fix the problem optimally locally?

actually yeah that's slightly better, becaues there are suboptimal fixes,
like blindly retrying...

consider what the omniscient, global solution to an error is,
and implement that.

this requires pushing errors upstream,
rather than doing suboptimal local fixes,
like logging the error and ignoring it.

title: implement the globally optimal solution to errors

(now I'm just saying, "be willing to fix things everywhere", heh)

(is that maybe what I really mean? hm....)
(like, that is, be willing to change other components to make the best system)

so, I can't say:
"prefer pushing errors upstream to fixing the problem locally"
because sometimes the optimal solution is to fix the problem locally...

and I want advice which always applies... hmm...
(I didn't bother thinking about that for completion/readiness stuff...
but then again, those are matters of performance, latency, memory usage)

so... if I want people to fix things everywhere...
that's an important post to link...

but taking that as a given...

I'm specifically telling them to implement the globally optimal solution - but wait,
that's not actually different from fixing things everywhere

the tao of being willing to change other components *is* to find global optimality.

so I guess, more specifically,
I'm saying the specific strategy of:
propagate errors up to the point where they can be best handled.

ooh, ooh, I like that.
some errors can be handled best where they're seen,
whereas others need to go up to be handled.
sure.

it would be nice to say, just abort, though.
like, that's a concrete strategy...

keep failing until you reach the level where the failure can be best handled.
keep failing up until you reach the level where the failure can be best handled.

OK  OK OK OK

let's think about what we'll actually say.

I don't think we need to worry about the notion of "failure" not exactly matching spinlocks...
it's fine, I suppose

propagate errors up to the point where they can be best handled.

send failures upstream to the point where they can be best handled

send failures upstream, don't retry

send failures upstream, to where they can be fixed
*** what about if fixing it is downstream?
    some failures are in, say, storage systems.

    well, I guess I just don't think there should be any such stuff!

    the user should pass in the caps for what storage system to use
    or things like that.

    it's always the user's fault.
    the user always needs to fix it.
    or, at least, react to it...?

    okaaaaaaaaaaaay fiiiiiiiine this is even more complex.

    so I could:
    - send failures upstream
    - send failures downstream
    - just retry

    is there any scenario I think "send failures downstream" is good?

    I mean, I guess "send failures downstream" is the same thing as "just retry"

    it's just, you should actually try to fix failures rather than blindly retrying;

    either send failures upstream, downstream, or fix it yourself.

    like. either notify your user that something is broken,
    notify your dependencies that something is broken (???),
    or fix something yourself...

    argh.

    maybe I should just simplify it to...

    "be willing to send failures upstream".
*** title: be willing to send failures upstream
    it is not your responsibility, nor should you even try,
    to paper over every possible failure.

    you should propagate failures upstream.
    there are many ways to do this:
    - respond to requests from your users with failures immediately (rather than retrying)
    - when your dependencies fail, abort your program so your users (the process supervisor) knows you've failed; don't just wait

    this makes your system more responsive to failures
    and avoids getting things stuck in a failed state.
** don't retry!!!
   like, okay.
   when can we retry?

   only if we actually fixed the issue.

   that's actually a pretty simple answer.

   (and what do you do if you can't retry?
   send the failure upstream!)
*** only retry if you've actually fixed the issue
    don't just retry in a loop, or sleep and retry in a loop.

    only retry if you've actually fixed the issue

    - in spinlocks you retry because you've counterfactually fixed it
    - in read_exact...

    okay.

    the real issue with my earlier statement
    "don't do suboptimal retrying or fixes yourself"
    is: "what is the globally optimal fix? how do I know it?"

    how do I know that read_exact is globally optimal?

    i mean, it's not clearly optimal...
    mmm hmmm mmm mm I suppose it should depend on the spec for read_exact.
    the possible failures...

    read_exact has a control effect, it blocks if there's not enough data

    the spec is that it returns exactly that much data...
    not a partial amount...

    also, an fd is no longer readable after wakeup;
    waiting again is optimal - why?

    why don't we fail up with an EAGAIN?

    well it's that it's not in the spec of these functions.

    if you have this kind of failure in your spec/signature,
    then don't retry to try to avoid it!
    just propagate it!

    on the other hand, if it's not in your signature...
    you have to retry to avoid it.

    so the question is, do you want to ever allow failure?

    basically:
    don't have some fixed number of retries and then failure;
    fail immediately or retry forever.

    okay, sure, so that's a given.

    but when do you propagate certain failures up and when do you not?

    if e.g. failure to store something can't be propagated...
    then you might just block forever while retrying...

    rather than allow the failures to propagate up

    okay, so then this is just a point about allowing failures to propagate.
**** POINTS ONCE MORE
     - either retry infinitely or not at all (allow failures to propagate upwards)
     - allow failures to propagate upwards

     okay so then the question is,
     when do you propagate failures upwards and when do you retry infinitely?

     i.e. when do you put some failure condition in your interface?

     well... different interfaces have different failure conditions...

     I guess, basically:
     expose as many possible failures in your interface as possible.

     read_exact handles exactly one failure

     retrying with EAGAIN handles exactly one failure.

     handle one failure at a time and propagate the rest.

     or, maybe not that, but at least,
     expose as many possible failures as possible.

     ya. ya ya ya.

     okay so:

     - expose as many failures as possible (push them out)

     but then, how do you know which failures to handle yourself?
     well...
     it depends on the application.

     the point of some application should be to handle some kind of failures...
     i guess?

     so, question:
     what failures should the application ahndle
***** kai response
      it depends on the cost of handling the failure...

      the only thing you can do is retry.

      (wait is that really the only kind of failure handling?)
      well we need to know all the details of retrying;
      i.e. we can't just retry on a broken connection that we didn't create.
      we need to know how to create

      kai says: should be on the outermost layer.

      so expose every failure that isn't totally internal?

      or, it should be handled at the outermost place that has the ability to handle it.

      kai: if you expose it too far up, aggregating errors can happen,
      where you retry too much and your failure recovery is inefficient
      e.g. retrying the whole pipeline

      okay so two things here:
      - handle the error at the outermost place
      - efficiency of handling an error (not retrying too much)

      kai attempt at totalizing rule:
      handle each error only once.
**** shower thoughts
     ok, I think I just resign to:

     - push failures up
       (be willing to just push a failure up instead of trying to handle it)
       (be willing to say "this will cause me to fail dramatically if it happens",
       that is, to include that failure in your spec.)
       (and also... that spec should describe upstream things.
       like put them in your upstream spec... rather than your downstream stuff...
       which is basically internal anyway.
       okay, "include the failure in your external-facing spec")
       (try to have a narrow set of functionality,
       which includes not trying to paper over every possible failure,
       but instead pushing those upstream)
     - don't just retry
       (don't just sleep and retry, fix the problem;
       if you can't fix the problem, push the failure out.
       and if you do retry, retry infinitely)
       (if a failure is in your spec definitely don't retry, just fail)

     just punt on the question of "when should you handle the failure yourself?"
     i.e. when should conceal a failure from your spec.

     right so, retries is fully determined right?
     if it's in your spec, don't retry;
     if it's not in your spec, retry forever, but also,
     make sure to actually fix the issue.
     (if you can't actually fix the issue, perhaps the failure should be in your spec,
     so that people upstream of you can see the failure)
*** title: prefer to propagate failures up
    Be willing to just push a failure up instead of trying to handle it.

    Be willing to include failures as part of the specification of your component
    (even if you don't actually have a specification).

    It is not your responsibility, nor should you even try,
    to paper over every possible failure.

    There are many ways to propagate failures up:
    - Allow exceptions to propagate rather than catching them
    - Respond to requests from your users with errors
    - Crash the service, so the process supervisor sees the process exit non-zero

    This makes your system more responsive to failures
    and avoids getting things stuck in a failed state.

    Only handle a small specific set of failures in your component;
    let the rest be passed through and handled by your user.
*** title: when handling a failure, don't just retry
    When you do handle a failure rather than propagating it up to your user,
    handle it right.

    Do something to fix the failure before retrying.
    Inside the body of your retry loop, do something like:
    - waiting for more data to be available, before trying to read again
    - waiting for more events to arrive, before running more event handlers
    - waiting for the service to be ready again, before sending another request
    - counterfactually releasing the lock, before trying to take the lock
      (link to spinlock game theory analysis)

    Don't busy loop; that is, don't just wrap a loop around:
    - do nothing and try again
    - sleep for an arbitrary amount of seconds, before sending another request

    If you don't do anything to fix the issue,
    it may never be fixed, or may be fixed purely accidentally,
    which means your retries will mask serious failures
    rather than letting them be appropriately handled.

    Busy looping wastes resources.
    Without busy looping, an idle process can be paused and paged out,
    so that it consumes no CPU time, memory, or energy
    until the operation it's waiting for completes.

    If you can't fix the issue,
    then you shouldn't be retrying at all;
    propagate the issue upstream.
