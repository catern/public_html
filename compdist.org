* the essence of distributed systems vs local systems is the single fault domain

A single machine is a distributed system with one failure domain

basically I just want to talk about all the things that are similar between the two

because I'm tired of confusion around this

similarities:

- cores communicate via message passing over a network
- latency between components is highly relevant for performance
- components run concurrently
- they don't have a single global clock ??? maybe??? buses are on different clocks right?

https://en.wikipedia.org/wiki/Distributed_computing
this is actually pretty good

A PC is like a distributed system in every way except one

A single computer is a distributed system in every way but one

https://thume.ca/2020/05/17/pipes-kill-productivity/
is a good article to debunk:

- Fragile: The network connection or the other end can have hardware
  failures, these have different implications but both manifest as
  just a timeout. Everything needs to handle failure.

  ah, but in hardware, we ultimately bottom out in "kill everything".
  that is the primary difference.

- Narrow: Bandwidth is limited so we need to carefully design
  protocols to only send what they need.

  yes...

- Laggy: Network latency is noticeable so we need to carefully
  minimize round-trips.

  yes...

- Asynchronous: Especially with >2 input sources (UIs count) all sorts
  of races and edge cases can happen and need to be thought about and
  handled.

  yes...

- Mismatched: It’s often not possible to upgrade all systems
  atomically, so you need to handle different ends speaking different
  protocol versions.

  Same with hardware:
  you have to be compatible with all kinds of various versions of protocols standards,
  because usually the computer is made out of components from a lot of different vendors.
  If a single vendor made everything,
  then they could atomically upgrade inside a box,
  the next time they brought down the whole box...
  but you could do the same if you were willing to bring down your whole distributed system.

- Untrusted: If you don’t want everything to be taken down by one
  malfunction you need to defend against invalid inputs and being
  overwhelmed. Sometimes you also need to defend against actual
  attackers.

  Same with a single machine: e.g. thunderbolt vulnerabilities

Most programmers ignore all these things, basically completely,
when writing software to run on a single machine.
I reject the claim that these things cannot be ignored

Indeed, I believe we cannot advance until we have figured out a way to abstract them away.

"I am determined to move beyond this way of interacting with systems"
https://twitter.com/rsnous
** title
The only difference between a distributed system and a single computer,
is the number of failure domains

There is no difference between a distributed system and a single computer

(the fault domain distinction is something we have created...)

yeah so let's dig into this fault domain thing

is it actually more of a end-user UI thing?

title: There is no essential difference between a distributed system and a single computer
title: There is no fundamental difference between a distributed system and a single computer
title: There is no fundamental difference between a distributed system and a single machine

maybe there are multiple fault domains in a single machine?

i guess it's just that...
generally if there's some severe error we power off the whole machine?

whereas...
we can't cut power to two computers in different place?
without blowing up the earth?

on the other hand...
multiple computers in a single datacenter, we can do exactly that...

if some node gets locked up,
we can just cut power...
both in a computer and out of it....

but how do we cut power?
well, we use some oher part of hardware, which could also fail!
maybe?

which is equivalent to having an admin network that would let us cut power to things...

yeah I just don't think there's any fundamental difference
cuz.... hmmm....

maybe I would emphasize this part as, the part that is most tricky to work with...

title: There is no fundamental difference between a distributed system and a single machine as they actually exist
title: Every computer has a distributed system inside it
title: Your computer has a distributed system inside

like I want to say, the reality of our machines, not in theory,
is that they are distributed systems

title: A single machine has all the same attributes of a distributed system, so why is the latter so much worse to program?

i mean, what's my answer?
we've been screwing up our approach to programming?

i mean, i do think there is a simpler approach...

I guess fault domains are something important here....
or at least that's what I've thought

okay. really I just want to say...

title: There is a distributed system inside your single machine

just like, basically...

convey all these interesting "hey here's a distributed system problem that crops up when programming a single machine"

the problems aren't *not real*, it's just that they're also in a single machine.

(and I think that justifies abstracting over them in both cases - or, shows that it's viable in both cases)

okay so yeah, basically, when programming a single machine you don't avoid it

"all the same distributed systems problems show up when programming a single machine"

"You cannot avoid distributed systems problems by sticking to a single machine"

except you can because then it's permisssible to abstract over them...

"Sticking to a single machine will not solve your distributed systems problems"

"You can't avoid distributed systems problems by sticking to a single machine"

"Distributed systems problems don't go away if you stick to a single machine"

yes... this is moving in the right direction...

the real point I want to make is about how sticking to a single machine is not the solution,
accepting more abstraction is the solution,
and we just already do that with single-machines.

but I want to make that indirectly.

"Single machines are just abstracted distributed systems"

oohh now this is a good line.

"A single machine is just an abstracted distributed system"

yes... this is good. and really this is what I want to say...

"A single machine is just an abstraction layered over a distributed system"
"A single machine is just an abstraction layered on top of a distributed system"

nah, this is pithier:

"A single machine is just an abstracted distributed system"

"A so-called single machine is just an abstracted distributed system"
"A single computer is just an abstracted distributed system"
"A single computer is an abstracted distributed system"
"A 'single' computer is actually an abstracted distributed system"
"A single computer is actually an abstracted distributed system"
"A PC or server is actually an abstracted distributed system"
"A typical single computer is actually an abstracted distributed system"
"A typical single computer is just an abstracted distributed system"

I like "just" over "actually"...
and "typical" is not necessary but maybe *something* replacing the subject...

"A single computer is just an abstracted distributed system"

Every machine is actually an abstracted distributed system

Each machine is actually an abstracted distributed system

Every machine is an abstracted distributed system

A "single machine" is an abstraction on top of a distributed system

Linux is a distributed system

(Can I just spit out the point?)

When you're writing a program that just runs on one machine to avoid the complexity of running on a distributed system,
you're actually still writing on top of a distributed system,
just one that's behind an abstraction.

A single machine is actually an abstracted distributed system

Each single computer is actually an abstracted distributed system

again, what's the point?

Inside every computer is a hidden distributed system
Inside every computer is an abstracted-over distributed system

https://barrelfish.org/publications/barrelfish_hotos09.pdf
lol

Your computer is already a distributed system. Why isn’t your OS?

okay so the intro here is saying exactly what I want to say

likely because i was inspired by it

Your computer is already a distributed system. Why isn't your distributed system a computer?


maybe I should just do another paper summary

Your computer is a distributed system

yes. that will be the title and we will summarize/extract most of it from the paper.


then... a followup that is:

Why isn't your distributed system a computer?

nah

Your computer is a distributed system,
so abstracting over distributed systems to create a programming model is viable,
and we can ignore all the horrible details of distributed systems.


hey isn't the original mostly appropriate for my PL iceberg list? hm

okay so the followup article is just...
We have proof that it's possible to create a productive programming environment
as an abstraction over a distributed system
that mostly removes having to worry about any of the distributed system details.
I reject the claim that abstracting over a distributed system will fail in all cases;
(e.g. "don't make rpcs look like local calls);
we have proof that it can work sometimes.

So let's find other ways to do it.
Distributed shared memory is an idea that has been tried but has failed,
so we need knew ideas.

I don't believe the coherent-shared-memory-multiprocessor
is the only model for abstracting over a distributed system that works.
Surely there are others; we just need to find them.

We have an existence proof that one can make a productive abstraction
on top of a distributed system
that saves programmers from having to wrestle distributed system problems:
the shared-memory-multiprocessor.
So I think we can make more.

We need to make an abstraction on top of large-scale distributed systems
which provides the ability to program that without caring about distributed systems issues.
We should completely abstract away the fact that it's a distributed system.
This is an old research area that is often condemned as a false idol,
given the many failures of attempts to do this.
But we've succeeded once:
the shared-memory-multiprocessor.
I think we can succeed again with a new model.

There are two directions one can go from there:
Scale up the shared-memory-multiprocessor,
or make a new model entirely.

I think we need a new model because I don't like the shared-memory-multiprocessor.

Honestly I think a good new distributed programming model
*also* will help us replace the shared-memory-multiprocessor in the small scale.

Yeah there's two conclusions you can draw from "your computer is a distributed system":
- your large-scale distributed system can be a computer (be abstracted)
- your computer abstractions in a small-scale distributed system,
  can be optimized with large-scale distributed system techniques;
  like message passing etc

So one is to add new abstractions,
and the other is to replace PC abstractions with distsys abstractions.
remove abstractions, in other words...

Yeah so:
the PC is an abstracted distributed system.
that suggests:
- we can add abstractions to other distributed systems to make it easier to program like the PC

- we can remove abstractions from the PC to make it more like other distributed systems
  (e.g. use message passing - link to flexsc.html)

SMP is the acronym we should use

yeah we can put this all in one article.


add abstractions to distsys,
remove abstractions from SMP

um okay. I just need to... write it now.
** just writing it
title: Your computer is a distributed system

Most computers are essentially a distributed system internally,
and provide their more familiar programming interface as an abstraction on top of that.
Piercing through these abstractions can yield greater performance,
but most programmers do not need to do so.
This shows that it is possible to successfully abstract away the distributed nature of a system.
# put this at the end actually? not in the intro? just stop intro here?
Perhaps we can develop new abstractions which are suitable both for individual computers
and for much larger distributed systems.

Many have observed that today's computers are distributed systems
implementing the abstraction of the shared-memory multiprocessor.
The wording in the title is from the 2009 paper
"[[https://barrelfish.org/publications/barrelfish_hotos09.pdf][Your computer is already a distributed system. Why isn’t your OS?]]".

Some points from that and oher sources:
- The latency between the CPU and main memory and other components is highly relevant.
  For example, rather than sending all communications through a central point,
  different components can use DMA to communicate directly.
- To reduce these latencies, we put caches in the middle to reduce the load on main memory.
- Those caches themselves are sophisticated distributed systems,
  implemented with [[https://en.wikipedia.org/wiki/Cache_coherence#Coherence_mechanisms][message passing]] between individual cores.
  # leases? have more of an analogy to HTTP?
- There's multiple cores running concurrently,
  e.g. graphics cards, storage devices, network cards,
  all running their own firmware running concurrently with the main CPU,
  and communicating over the internal network (e.g. a PCI bus).
- Components can appear and disappear and fail independently,
  and other components have to [[https://linux.die.net/man/8/hotplug][deal with this]].
- Components can be malicious and send adversarial inputs;
  e.g., plugging in a malicious [[https://en.wikipedia.org/wiki/PlayStation_3_Jailbreak][USB]] or [[https://en.wikipedia.org/wiki/Thunderspy][Thunderbolt]] device
  is not supposed to give the attacker root.

# what about "network partitions"
# simultaneous updates? nah
# there's many different programs

But by reasoning explicitly and using distributed systems techniques,
we can get better performance.
- Message-passing instead of communicating via shared memory is [[https://catern.com/flexsc.html][faster]].

# um what else...
# I guess we just need to break that down

It is also commonly said that you can't abstract over the network.

https://en.wikipedia.org/wiki/Fallacies_of_distributed_computing

oh this is relevant:
https://en.wikipedia.org/wiki/PACELC_theorem
cause it talks about how consistency has a latency cost

In distributed systems,
it is often supposed that you cannot abstract away the issues of distributed programming.
One classic and representative [[https://www.win.tue.nl/~johanl/educ/2II45/2010/Lit/Tanenbaum%20RPC%2088.pdf][quote]]:

#+begin_quote
It is our contention that
a large number of things may now go wrong
due to the fact that
RPC tries to make remote procedure calls look exactly like local ones,
but is unable to do it perfectly.
#+end_quote

Nevertheless, today, essentially all programs are written
in an environment that abstracts away the issues of distributed systems.
They are filled with RPCs which appear as if they're local operations.
Accesses to memory and disk are synchronous RPCs to relatively distant hardware,
which block the execution of the program while waiting for a response.
Indeed, it's essentially impossible on current architectures to write a program
which explicitly sends and receives data from memory in an asynchronous style.

This offers hope that we can develop abstractions
which can scale up to larger distributed systems.

Perhaps we can develop an abstraction which is suitable both for individual computers
and for much larger distributed systems,
and explicitly and formally use the same techniques in both places.
One [[http://www.cs.cmu.edu/~tom7/papers/modal-types-for-mobile-code.pdf][possibility]] is to build a notion of "location" explicitly into our programming languages,
and statically track where each expression "is located".
Computation in different locations, and communication of data between those locations,
could be explicitly described with a single program.
This would be a powerful abstraction both for the lowest-level hardware implementation
and the highest-level globe-spanning distributed system.
