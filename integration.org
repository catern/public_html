#+title: No more DSLs: Implement and deploy a distributed system with a single program
#+HTML_HEAD: <style type="text/css">pre.src {background-color: #303030; color: #ffffff;} pre.src-python:before { color: #000000; }</style>
* Introduction
  :PROPERTIES:
  :CUSTOM_ID: introduction
  :END:
If you want to write a distributed system,
then instead of writing a thousand different programs and configuration files in many different DSLs,
you can use an approach I call "single-program systems",
and write just one program.
In this article, I describe how we do this at $DAYJOB,
and I [[http://catern.com/constructive.html][show]] some example programs.
These examples are in Python,
but this approach works in any language.

Besides single-program systems,
[[file:list_singledist.html][distributed languages]] also try to allow the user to
implement a distributed system as a single program.
But these languages assume the presence of lots of distributed systems infrastructure
which is maintained outside the language;
so a program written in such a language is only one component in a larger distributed system.
The single-program system approach, instead,
allows you to write a program which manages *every* aspect of the distributed system above the hardware level,
such that running the program deploys the distributed system,
and the program and the distributed system can really be said to be the same thing.

At $DAYJOB, each team maintains Python libraries for their components;
these libraries are used to write the single-program system.
All the component libraries are maintained in a single repository to reduce compatibility issues.
Some components are in Python,
but others are written in other languages (mostly C/C++/Java)
and run in a separate process.
In this way, performance-sensitive components of a distributed system are written in faster languages than Python,
in the same way that Fortran libraries are used to speed up math in Python programs.
A component might communicate with other components, replicate and persist state,
or any of the usual distributed system stuff.

The component libraries
use normal programming language features, including Python 3's static type system, to [[http://catern.com/progsys.html][handle distributed systems issues]].
For example, regular function arguments are used to express service discovery and dependencies between services;
for each service, there's a service-starting function which takes that service's dependencies as arguments,
uses those arguments to run the service,
and returns a value that can be passed as an argument (a dependency) to other service starting functions.
In general, services are "configured" by passing arguments, [[http://catern.com/config.html][in normal code]],
and multiple instances can be created just by calling functions multiple times with different arguments.

When a service-starting function starts a C/C++/Java process,
it starts the process directly as a subprocess, on some host,
which the program will continue to monitor directly
rather than delegating to a [[http://catern.com/supervisors.html][process supervisor]] service.
Functionality for distributed process spawning and monitoring
is shared through [[http://catern.com/services.html][libraries]] rather than by delegating to external orchestration systems,
making a single-program system completely self-contained.
This allows a single-program system to be used for deployment in any environment,
whether that's bare-metal, virtual machines, containers, or a developer machine;
though different environments and use cases usually implies different programs with different [[http://catern.com/config.html][configurations]].

You can run the resulting program to [[http://catern.com/run.html][run the full system]] for production,
or for testing the system [[http://catern.com/usertests.html][against its users]].
This program is not a DSL;
it doesn't get processed by some tool and turned into a list of actions to perform.
Rather, it's run by a normal Python interpreter,
and when the program calls =start_foo=,
the program starts up the =foo= process, right then, directly.
When everything is started,
the program continues running, [[http://catern.com/supervisors.html][monitoring each process]]
and providing interfaces (the Python REPL, UIs, etc)
to [[http://rsyscall.org/wish/][interact]] with the running system.

Upgrading the distributed system is a matter of upgrading the single program.
Doing this doesn't require downtime.
One simple approach is to restart the single program without killing its child processes,
and then the new version takes on the responsibility of gradually upgrading the services.
Alternatively, more advanced techniques such as [[https://en.wikipedia.org/wiki/Dynamic_software_updating][dynamic software updating]] 
could be used to upgrade the single program without restarting it.

This approach works for almost all distributed systems.
Most distributed systems,
such as those operated by a single company,
have centralized authority;
by this I mean that, ultimately, the owners and employees of the company
have the authority to do anything they want with the distributed system
(though doing anything actually useful might be [[https://www.lesswrong.com/posts/Wa2hASzbxyvutHJff/total-horse-takeover][hard]]).
A single-program system extends this to [[https://www.cs.princeton.edu/courses/archive/fall13/cos597E/papers/sdnhistory.pdf][logically centralized control]];
that centralized authority is delegated to a single program which controls the system.
Systems with centralized authority are easier to operate [[https://en.wikipedia.org/wiki/The_Nature_of_the_Firm][in many ways]],
but centralization isn't always possible or desirable.
Decentralized systems such as the Internet
couldn't be run as a single-program system
because they contain many interacting "ultimate authorities" running their own [[https://en.wikipedia.org/wiki/Autonomous_system_(Internet)][subsystems]];
each of those subsystems, however, could be run as a single-program system.

# Maybe don't use TestCase boilerplate?
# nah I think that's good...
* Writing a single-program system
  :PROPERTIES:
  :CUSTOM_ID: running
  :END:
We'll see this in action by looking at a working example program.
** orderd: an order entry daemon
   :PROPERTIES:
   :CUSTOM_ID: orderd
   :END:
- Accepts or rejects orders sent over a TCP listening socket
- Updates the =positiond= service with the positions
- Stores order data in a SQLite database

=orderd= is a real daemon ("d" is [[https://unix.stackexchange.com/questions/72587/why-do-some-linux-files-have-a-d-suffix][short for "daemon"]]), with a few details removed.
We're looking at =orderd= specifically
because it has only the three dependencies we've already mentioned.
Note that =orderd= itself may or may not be written in Python
this is abstracted away from us.
(although in practice, it is in fact a separate subprocess)

For our example, we'll write a test of our distributed system.
We'll start up =orderd= and its dependencies (just =positiond=) for the test,
using functions from the component libraries to run each service.
First, some testing boilerplate:
#+begin_src python
import unittest
from orderd import start_orderd

class TestOrderd(unittest.TestCase):
  def setUp(self) -> None:
    # TODO start up orderd and its dependencies
    self.orderd = start_orderd(...)

  def test(self) -> None:
    self.assertTrue("Do test stuff")
#+end_src

To write =setUp=,
we'll proceed by looking at the signature of the =start_orderd= function,
provided by the =orderd= component library.
Note the [[https://mypy.readthedocs.io/en/stable/cheat_sheet_py3.html][type annotations]] for static type checking, introduced by Python 3.

#+begin_src python
# in the "orderd" module
async def start_orderd(
  nursery: trio.Nursery,
  thread: rsyscall.Thread,
  positiond: positiond.Positiond,
  listening_sock: rsyscall.FileDescriptor,
  database: orderd.Database,
) -> Orderd:
#+end_src

We'll look at the =start_orderd= signature line by line,
creating each argument individually,
and at the end we'll call =start_orderd= and have a running instance of =orderd=.

The first three lines of the function signature 
(up to and including =thread: rsyscall.Thread,=)
are essentially common to all service starting functions.
The last four lines 
(starting with =positiond: Positiond,=)
are specific to =orderd=.
** =async def start_orderd(=
   :PROPERTIES:
   :CUSTOM_ID: async_def
   :END:
#+begin_src python
async def start_orderd(
#+end_src

=start_orderd= is an async function.
In Python, this simply means that it can run concurrently with other functions,
which allows us to start services up in parallel,
using Python-specific techniques which we won't show in this example.
Other than that, it's a completely normal function,
which is called with =await start_orderd(...)= from any other async function,
and which blocks execution until it returns.

Since =start_orderd= is async, we need to run it from an async runner.
We'll use the open source library [[https://trio.readthedocs.io/][=trio=]] for that,
which means we'll need to tweak our boilerplate slightly to use =TrioTestCase=.

#+begin_src python
from trio_unittest import TrioTestCase

class TestOrderd(TrioTestCase):
  async def asyncSetUp(self) -> None:
    self.orderd = await start_orderd(...)
#+end_src

Other than this change in boilerplate,
Python async functions work like any others;
you can safely ignore the "async" and "await" annotations.
We won't use any async features in this =TestCase= example;
the only use of async features will be later, with =start_exampled=,
when we look at how a component library is implemented.
** =nursery: trio.Nursery,=
   :PROPERTIES:
   :CUSTOM_ID: nursery
   :END:
#+begin_src python
  nursery: trio.Nursery,
#+end_src

[[https://trio.readthedocs.io/en/stable/reference-core.html#trio.Nursery][=trio.Nursery=]] is defined by the open source [[https://trio.readthedocs.io/][=trio=]] library,
and it provides the ability to start up functions in the background.
We pass it in to =start_orderd=
so that =start_orderd= can start a function in the background
to monitor the running =orderd= process.
If the =orderd= process exits, the background function monitoring that process will throw,
and the resulting exception will be propagated to the =trio.Nursery=,
which will deal with it in some way specific to how the =trio.Nursery= was produced.
Upon seeing an exception in a background function,
the logic for a =trio.Nursery= might call =start_orderd= again immediately,
it might kill the other background functions and start them all up again with =start_= functions,
or it might ultimately prompt for operator intervention [[http://rsyscall.org/wish/][through various means]].
An operator might then work at a UI or a REPL to fix the issue,
by calling =start_orderd= with different arguments.

In this case, we'll use =self.nursery= as provided by =TrioTestCase=,
which turns any failure in a background task into a failure of the whole test.

#+begin_src python
  async def asyncSetUp(self) -> None:
    # self.nursery provided by TrioTestCase
    self.orderd = await start_orderd(
      self.nursery,
      ...,
    )
#+end_src
** =thread: rsyscall.Thread,=
   :PROPERTIES:
   :CUSTOM_ID: thread
   :END:
#+begin_src python
  thread: rsyscall.Thread,
#+end_src

[[http://rsyscall.org/rsyscall/][=rsyscall.Thread=]] is defined by the open source [[https://github.com/catern/rsyscall][=rsyscall=]] library,
and it provides the ability to run system calls, including running subprocesses.
We pass it in to =start_orderd=
so that =start_orderd= can start the =orderd= subprocess,
as well as perform other operations to prepare the environment for =orderd=.
An =rsyscall.Thread= may operate on a local or remote host,
or inside a container or VM, or on other kinds of nodes,
depending on how the =rsyscall.Thread= was produced,
but it provides a completely common interface regardless of where it runs.

Component library code itself never runs distributed across multiple nodes;
there's a single Python interpreter on a single host.
All distributed operations are performed by method calls on =rsyscall.Thread= objects.

In this case, we'll use =local_thread= imported from =rsyscall=
and assigned to =self.thread=.
=local_thread= runs on the same thread as the Python interpreter - that is, on localhost.

#+begin_src python
from rsyscall import local_thread

  async def asyncSetUp(self) -> None:
    self.thread = local_thread
    self.orderd = await start_orderd(
      ..., self.thread, ...,
    )
#+end_src
** =positiond: Positiond,=
   :PROPERTIES:
   :CUSTOM_ID: positiond
   :END:
#+begin_src python
  positiond: Positiond,
#+end_src

This is the first =orderd=-specific argument.

=positiond= is a service which =orderd= updates with information about its position.
All the information required to connect to and use =positiond=
is contained in the =Positiond= class.

Since =positiond= is its own service, we need to use =start_positiond= to start it.

#+begin_src python
async def start_positiond(
  nursery: trio.Nursery,
  thread: rsyscall.Thread,
  workdir: rsyscall.Path,
) -> Positiond: ...
#+end_src

The first two arguments are shared with =orderd=.
The third argument, =workdir=, is unique to positiond.
=workdir= is a path in the filesystem that =positiond= will use;
in this case, =positiond= will use it
to store shared memory communication mechanisms and persistent data.

We'll pass a path in a temporary directory in this example.
#+begin_src python
    # Make a temporary directory
    self.tmpdir = await self.thread.mkdtemp()
    self.orderd = await start_orderd(
      ...,
      await start_positiond(self.nursery, self.thread, self.tmpdir/"positiond"),
      ...,
    )
#+end_src
** =database: orderd.Database,=
   :PROPERTIES:
   :CUSTOM_ID: database
   :END:
#+begin_src python
  database: orderd.Database,
#+end_src

This is a completely conventional SQLite database, initialized with the orderd schema.

Here, for a test, we're calling =orderd.Database.make= to make a fresh database, every time.
If we wanted to persist state between runs of =orderd=,
we'd pass in a =orderd.Database= instance from a previous run,
recovered from some known path in the filesystem with =order.Database.recover(path)=.

#+begin_src python
    self.orderd = await start_orderd(
      ...,
      await orderd.Database.make(self.thread, self.tmpdir/"db"),
      ...,
    )
#+end_src
** =listening_sock: FileDescriptor,=
   :PROPERTIES:
   :CUSTOM_ID: listening_sock
   :END:
#+begin_src python
  listening_sock: FileDescriptor,
#+end_src

This is a listening socket,
passed down to the =orderd= subprocess through file descriptor inheritance,
and used to listen for TCP connections.

This is standard Unix socket programming, so we won't go into this in depth;
although note that we create this with =self.thread=,
so that it it's on the same host as =orderd=.

#+begin_src python
  async def asyncSetUp(self) -> None:
    # Make a TCP socket...
    sock = await self.thread.socket(AF.INET, SOCK.STREAM)
    # ...bind to a random port on localhost...
    await sock.bind(await self.thread.ptr(SockaddrIn(0, "127.0.0.1")))
    # ...and start listening.
    await sock.listen(1024)
    self.orderd = await start_orderd(
      ..., sock, ...,
    )
#+end_src

** =) -> Orderd:=
   :PROPERTIES:
   :CUSTOM_ID: return_value
   :END:
#+begin_src python
) -> Orderd:
#+end_src

Like all good component libraries,
=start_orderd= returns an =Orderd= class
which contains all the information required to connect to =Orderd=,
such as an address and port, a shared memory segment, or a path in the filesystem.

=start_orderd=, again like all good component libraries,
will only return when the =orderd= communication mechanisms have been fully created,
and therefore the =Orderd= class can be [[http://0pointer.de/blog/projects/socket-activation.html][immediately used to connect]] to =orderd=.

** Full example
   :PROPERTIES:
   :CUSTOM_ID: full_example
   :END:
Here's the full, working example:
#+begin_src python
class TestOrderd(TrioTestCase):
  async def asyncSetUp(self) -> None:
    # self.nursery provided by TrioTestCase
    self.thread = local_thread
    self.tmpdir = await self.thread.mkdtemp()
    sock = await self.thread.socket(AF.INET, SOCK.STREAM)
    await sock.bind(await self.thread.ptr(SockaddrIn(0, "127.0.0.1")))
    await sock.listen(1024)
    self.orderd = await start_orderd(
      self.nursery, self.thread, 
      await start_positiond(self.nursery, self.thread, self.tmpdir/"positiond")
      await Database.make(self.thread, self.tmpdir/"db"),
      sock,
    )
#+end_src

Then we can proceed to [[http://catern.com/usertests.html][test by running user code]].

* Implementation of component libraries
  :PROPERTIES:
  :CUSTOM_ID: implementation
  :END:
Now we'll step through a working example of how a component library is implemented.
This one shells out to a separate process, =exampled=.

This daemon is packaged and deployed with Nix;
at $DAYJOB we use a proprietary package manager with similar APIs.

Below is the full code for the =exampled= component library,
with comments inline to explain it.
# Going line by line here in comments, rather than by arguments,
# because this is an implementation of an abstraction,
# not just a manipulation of abstractions

#+begin_src python
import nix_rsyscall
import rsyscall
import trio
# a Nix-specific generated module, containing the information required
# to deploy the exampled package; generated by setup.py.
import exampled._nixdep

class Exampled:
    def __init__(self, workdir: rsyscall.Path) -> None:
        self.workdir = workdir

async def start_exampled(
    nursery: trio.Nursery,
    thread: rsyscall.Thread,
    workdir: rsyscall.Path,
) -> Exampled:
    # deploy the exampled package and its dependencies; this doesn't deploy the
    # package for this Python library, but rather the exampled daemon
    package = await nix_rsyscall.deploy(thread, exampled._nixdep.closure)
    # build the command to actually run
    command = package.bin('exampled').args("--verbose", "--do-stuff-fast")
    # make the thread that we'll run that exampled command in;
    # this child_thread is a process under our control, see http://rsyscall.org
    child_thread = await thread.clone()
    # change the CWD of the child thread; CWD is inherited over exec, so it will be used by exampled
    await child_thread.mkdir(workdir)
    await child_thread.chdir(workdir)
    # exec the command in the child thread; this exec helper method returns a monitorable child process object
    child_process = await child_thread.exec(command)
    # monitor the child process in the background; see https://trio.readthedocs.io/
    # we'll get an exception if it exits uncleanly; this is our one use of async features.
    nursery.start_soon(child_process.check)
    # return a class containing exampled's communication mechanisms;
    # it communicates with the world only by creating files under `workdir'
    return Exampled(workdir)
#+end_src
* Conclusion
  :PROPERTIES:
  :CUSTOM_ID: conclusion
  :END:
A single-program system implements an entire distributed system as a single program,
delegating to libraries to share functionality and subprocesses to improve performance.

The alternative is writing many programs and many configuration files in many different DSLs:
Kubernetes, Helm, Terraform, Ansible, systemd, cron jobs,
shell scripts, configuration files in JSON, TOML, YAML, CSV, INI, etc, etc, etc.
I think the advantages of the single-program approach are self-explanatory.

The techniques I use for single-program systems
are explained in greater detail in other articles linked in the introduction.
With those techniques,
and with the open source libraries [[https://github.com/catern/rsyscall][rsyscall]] and [[https://trio.readthedocs.io/][trio]],
anyone can write a single-program system.

If you're interested in working on such systems at $DAYJOB, we're hiring; [[./tsint_job.html][here's a job description]].
* notes :noexport:
ok so we want the big example section still,
with the concrete code... hmmm....

we'll call it... toplevel?

yeah I still like the name toplevel

so the title can be something like...

Toplevel: A library for running


maybe I should say...

a library and style?

a style... for running distributed systems programmatically

coding standards?
an approach?

a way?
a path?
a paradigm?

paradigm is a bit pretentious

also the existing paper

a pattern???
I like pattern
a pattern for deploying distributed system programmatically

well!
I really am talking about the library of functions here.

I've already explained the pattern

I'm just talking about the library

i'm worried about my explanation style here...

maybe I should have the concrete examples to explain the theory,
instead of going through the dependencies one by one?

MAYBE I should just unite them?

Should I just extend the distributed systems article!??

Yeah! that actually would be good.


hmm it does make the article really long though...

maybe I should just cite it quickly, with the core point of,
expressing dependencies as arguments?
not the further examples of type parametrization and multiple environments?
yeah yeah that seems good.

so I can just reproduce that explanation


okay and so, we can show that we're looking at a test of orderd,
because it's completely self-contained; (a test is traditionally completely self-contained)
we don't have to posit any external hosts or nodes that we'll run orderd on,
we just do it totally self contained.

yeah,
"we could substitute more sophisticated values for thread and nursery here,
to get more sophisticated behaviors".


okay so:
- ???
- dependencies as arguments, link to "type systems for deploying distributed systems"
- ???
- "we'll look at a test because it's self-contained and a clean slate;
   we don't have to assume we have other multiple hosts that we'll use,
   and we don't have to worry about using persistent storage for data storage."
- example with "orderd"

So I guess I won't start by talking about tests;
we'll introduce tests later on as an example.

We'll start with a link to the distributed systems thing.
And also run your system. and code as config...

hmm.

or maybe not? we'll explain it more anecdotally;
at my job we have a library,
built along the lines blah blah,

or, wait. I want to say it concisely.
** post
At $DAYJOB we have 
a sophisticated collection of libraries for running components in our distributed system,
collectively called "integration libraries".
I describe them here as a constructive proof of (link constructive proof post)
the theory I've described elsewhere. (link each word)

- We use it to run our system, which is very important.
- A brief summary of the theory. [blah blah dependencies as arguments]

- The libraries consists of a collection of functions

** thoughts
okay so...
can we just say that we need to run our distributed system?

I mean, do we need to explain why we can't use other things?
maybe we don't need to do that, hm.

yeah I don't think we need this justification section,
which explains why we don't use kubernetes etc
(that can be... underdefined... and let people draw their own conclusions)

oho!
I can link "constructive proof" in the introduction,
to my new constructive proof article!

so no discussion of the justification versus other systems;
we'll just say, it's for running our system,
citing the "run your system" post.

well, kai says we should get to the point quickly.

I guess we can have one sentence about running the system...
at the start?
yeah and include a link to usertests too.

the key important prep is that dependencies are arguments;
we'll inline just that,
then we'll go into the actual example.

right so...

* title :noexport:
come up with a title!

Running a system with types in practice?

okay. so...

"Running a distributed system with "integration libraries""

or...

Running a distributed system programmatically

Infrastructure as code?

what actually is it?

it's an example of everything.

A real example of..

Running a distributed system, in practice

A concrete program which runs a distributed system

Distributed deployment with Python
Practical distributed deployment with Python
Concrete distributed deployment with Python

I should mention Python because that makes it clear these are real programs,
in a real language,
with real libraries.

Deploying a distributed system with Python
Examples of deploying a distributed system with Python
Examples of distributed system deployment with Python

we want to make sure that it's clear that the distributed system is not written in python

Using Python to deploy a distributed system
Examples of using Python to deploy a distributed system
An example of using Python to deploy a distributed system
Concrete usage of Python to deploy a distributed system
Concrete usage of a real language to deploy a distributed system
No more YAML: Using Python to deploy a distributed system


Yeah a tagline might be good

No configs: Using Python to deploy a distributed system

except it's not no configs,
that's what supernet does, it also uses python AFAIK,
the tricky part is...
well it just configures a bunch of services in a haphazard way,
with no guarantee that services are actually correctly configured.

Maybe, using *typed* Python?

Using typed Python to deploy a distributed system correctly
Using richly-typed Python to deploy a distributed system correctly
Using richly-typed Python to deploy a distributed system

hmmmm....

No more DSLs: Using richly-typed Python to deploy a distributed system

yeah that sounds kind of good.
but, we want to be a little more concrete.

like... hmm...

we want to express that it's a concrete example...
but I don't know if that's necessary?

I mean, my previous article was just saying,
use a type system.
and it had a python pseudocode example.

Oh how about:

No more DSLs: Using richly-typed Python to deploy a distributed system directly

well, I mean, that's just what we're doing I guess.
which I've outlined in previous articles, which I'll link...

yeah okay. I mean, this title integrates all those other articles.

and is concrete: this is Python, this is a distributed system.

what about:

Using richly-typed Python to deploy a distributed system directly
Using richly-typed Python to deploy a distributed system directly, no DSLs
Using richly-typed Python to deploy a distributed system directly, without DSLs

No I like the initial little tag, "No more DSLs".

and the article is the constructive proof;
linking to other articles to explain the theory.

okay, so that's a good preliminary title:

No more DSLs: Using richly-typed Python to deploy a distributed system directly

hmm it's kind of long

No DSLs: Using richly-typed Python to deploy a distributed system directly
No DSLs: Using typed Python to deploy a distributed system directly
No DSLs: Using typed Python to deploy a system directly
No more DSLs: Using typed Python to deploy a distributed system directly
No more DSLs: Using typed Python to deploy distributed systems directly
No more DSLs: With typed Python, deploy distributed systems directly
No more DSLs: Using typed Python to deploy distributed systems directly

yes, there we go:

No more DSLs: Using typed Python to deploy distributed systems directly

** explain no more DSLs?
I guess with that little "No more DSLs" tag,
I should maybe explain the alternative to Kubernetes...

that's probably another article yet again.

oh but I can just link the libraries instead of services article!

is there anything in there that isn't part of libraries vs services?

I guess open source exit vs voice, specifically the section about making forking easy,
that services make that hard.

but, still...
it's not just "No more DSLs",
it's also "No more orchestration/deployment services".

which is kind of implied in libraries vs services,
and also in the process superivors article.
yeah I don't think I need to explain that.
it's kind of related to SDN though.
** DONE make small excerpt explaining "direct" as mentioned above
I might have one excerpt though;
in addition to the "typed" paragraph,
I'll talk about "direct".
that should cover it, yeah.

and I can link the supervisors and libraries vs services articles there.

"instead of writing a library to generate configs to configure a service which deploys your services,
write a library to deploy your services."

"instead of making a service to deploy services, write a library to deploy services"
** maybe explain that we deploy on VMs, and so-called "bare metal"?
not containers?

I might mention that in the threads section...
but maybe not...
just might make it a little more explicit that we're mutating the Nix store?

well, we'll just see how beta readers take it.
** for virality
I need to keep "No more DSLs" in the title
* thoughts :noexport:
Should I justify writing a test?

- "we'll look at a test because it's self-contained and a clean slate;
   we don't have to assume we have other multiple hosts that we'll use,
   and we don't have to worry about using persistent storage for data storage."

but maybe not?

well, I don't have to justify it if I make a real example of running it elsewhere after that.

eh it's good enough

* TPS redesign :noexport:
  hey maybe I could merge the iqueue and the database?

  what I could do, maybe...
  is put the JSON after the delta message in the iqueue.

  I'm guessing posdelta won't mind that...

  just, that would be nice because,
  that would get rid of one of the stateful arguments


* TODO note the two advantages of passing dependencies as arguments :noexport:
  startup ordering and service discovery
* more notes :noexport:
maybe we should do the old style with an integration class?

actually the integration class might be confusing.
** TODO real prod example
we should probably have a real prod example though.
that will probably clear things up...

and prevent confusion...
** DONE link libraries vs services, that explains why not to use Kubernetes
link it in the links section, that is
* feedback :noexport:
** boopy
The writeup doesn't have much...
The writeup is kind of just explaining the code, no interpretation
*** formatting issues
White squares???? (fixed with =pre.src-python:before { color: #000000; }=)

Weird framing when you shrink the screen?
** tm007
*** DONE my reaction
it seems like he's concerned about compatibility?

the issues inherent in having this split across multiple libraries?

maybe I can just say there's one library...

or, say that they're maintained in a monorepo?

that might be a more direct way to say it...


okay I think I can add a line in the introduction saying,
"They're maintained in a single repository to reduce compatibility issues."

but this increase compatibility issues with your own components!
but, meh, maybe people are less likely to notice that.
** Logos01
*** my reaction
he's saying I'm not explaining the problem

that sounds kinda like tm007's initial confusion about what the point was

yeah okay that's fair and true.

tm007 was also asking about the domain...

maybe the conclusion should say, this approach is applicable everywhere.
and summarize again the links in the introduction;
maybe rewording them to talk more about the problem.

Logos01 didn't really read it tho...

maybe I should reword the second paragraph to emphasize the purpose first:
To run the system, for production or testing.
yes, that would be perfect.
** nedbat
<nedbat> i'll take a look
> thank you! much obliged, sorry for the repeated message heh. here it is, any feedback is appreciated: http://catern.com/integration.html
<nedbat> i think your introduction could use more of an introduction.
<nedbat> i had to wrap my head around the idea that there are interlinked essays here, and this piece claims to validate the others, but i haven't seen the others yet.
> let me say right away that they aren't necessary context, no need to look at them
<nedbat> that's what i would have thought, but you say this essay's whole point is to support those, sort of.
<nedbat> "First some unittest boilerplate for the test: "  what test? I thought we were starting up services. that could use some connective text.
<nedbat> you say that the async stuff isn't really relevant.  could you literally remove all the async parts, to truly focus on the parts you want to focus on?
<nedbat> "capability" is new to me, and I wonder how important it is to your points.
> (just want to say all this feedback so far is great, thank you)
<nedbat> is your conclusion something like, "Orchestrating services is often done with bash or dockerfiles, but you can use typed Python functions to achieve the same effect, with better testability"?
<nedbat> btw, capabilities might be something you still want to reference, but again you can do it in a way that gives people permission to skip over it if they want
> thanks, I'll just remove capabilities - you're right that it's irrelevant
> just to be clear, is that your understanding of the point of the article?
> ("is your conclusion something like")
<nedbat> catern: that's what I'm getting from it.
> yes, that's (a key of) the conclusion
> key part of*
<nedbat> if you put that statement in the introduction, it will help people understand where you are headed, and why you are showing them this
> yes, I'll definitely work on incorporating that and the rest of this feedback
> thanks again for reading it!
<nedbat> ok! :)
*** my reaction
great feedback, great...
** amogorkon
<amogorkon> catern, i would refrain from using "cute" names, why not full words?
> amogorkon: er, sorry, I don't know what you mean
<amogorkon> "orderd"
<graingert> isn't that a daemon that does orders
<amogorkon> positiond
> amogorkon: ah, I see, thanks for the feedback
<amogorkon> yw
*** my reaction
hmm yeah maybe this is true.

hMmmMMmmmMmmmm yeah it's not immediately familiar to people

so maybe I'll give them other names...

or just call it `order_daemon`

ohHH HmMMMM

they aren't really daemons though because they're running in the foreground... kinda...

hmmmmMMMMm
** corbin simpson
*** my reaction
he seems to not fully comprehend the dependencies-as-arguments approach



maybe I should clarify that there's no distributed Python execution going on?


(but Corbin can't understand anything. so...)


but yeah, definitely I should say that there's no distributed execution.

maybe in the rsyscall.Thread section?
** feep
<feep> catern: I kinda stopped reading because it seemed *inapplicable*
<feep> it seemed like the sort of article I'd read if I was interested in deploying services with python, and ... I'm not

...
<feep> catern: um, you misunderstand me, the problem wasn't the python
<feep> the problem is that I to a first approximation don't care about distributed systems.
> oops, very well
<feep> python probably doesn't put people off fwiw
<feep> or like ... distributed deployment, I guess, to put better
> nah I still think this is a good change
<feep> there's people into that sort of integration stuff, I'm just not one~
> feep: what if I was to include a line along the lines of "this is a new object-capability secure way to write distributed systems, like existing distributed languages such as Erlang and E but supporting any language"
<feep> I'm ... like, the problem isn't that I don't understand the reason why I should think this is cool
<feep> catern: you're just giving me more evidence why I don't find the topic of the article interesting :p
> feep: no no that's fine, you just corrected "I don't care about distributed systems" to "I don't care about distributed deployment", so what if I told you that it's really about the former, than the latter
<feep> oh hm
<feep> hm
<feep> idk, I just sort of bounced off
<feep> hm
<feep> catern: okay, I tentatively grant that this should maybe interest me, I just bounced off the topic around the time you started pulling in python libraries
> yes that's fine, it's kind of oriented towards someone who *does* care about distributed deployment right now, but that's not necessarily good :)
<feep> orient it towards people who care!
<feep> don't try to pivot to people who aren't interested, that kills articles (and browsers, cough firefox cough)
<feep> I feel the right way to do stuff is to fully commit to people who are interested back
*** my reaction
I'm guessing this is the title

and also the repeated mention of Python

maybe I should say, Python isn't important...

oh 
hm
it wasn't the python, it was the distributed deployment.

maybe I need to change the title

Writing distributed systems 

Writing distributed systems with 

Writing distributed systems without DSLs, without 

Write a distributed system as a single richly-typed directly-runnable program

yeah okay, much more focused on writing distributed systems now...
** dbohdan
<dbohdan> catern: Even after rereading, I have questions about your distributed system.  I know it isn't what the article is about, but I am left wanting more context.
> dbohdan: thanks for the feedback about the supervisors, and hmm I'll try and think of a way to deal with the code blocks...
> dbohdan: ah, thanks for the feedback. what questions do you have? and, if you don't mind, could you summarize very briefly your understanding of the article?
> (if you feel you can)
<dbohdan> catern: For example, are there multiple instances of each component in your system?
If there are, how do they interact/are they prevented from interacting?
Is this a something you deploy to physical servers, containers, or some cutting-edge cloud nonsense?
For me the deployment code is left hanging in the air without being able to imagine these things more concretely.
<dbohdan> "Prevented from interacting" is re: your use of SQLite
*** my reaction
great questions!!!

okay, so I should probably say explicitly we use this to deploy to... all three!
physical servers, virtual machines, containers, and cutting-edge cloud nonsense if you want it.

and, multiple instances...
we should express that different instances can be configured for different 
** xkapastel
<xkapastel> i guess the idea is to absorb all those special purpose languages in
to one host language with nice integration between everything
...
<xkapastel> i'm not an expert on writing or anything so i don't want
to boss you around, but what i would do is take this sentence > By
using multiple such libraries together, one can write a distributed
system as a single program, and run that program to run the full
system, for production or for testing the system against its users.
*** my reaction
lol he says this right after I delete "No more DSLs"

okay so I clearly should add some more contrasts

maybe at the end of the introduction?

These techniques allow a distributed system to be treated holistically as a single program.
There are purpose-specific distributed languages which have the same goal,
but they generally depend on substantial amounts of existing infrastructure:
there is no single program you can run to run the distributed system,
you have to deploy large amounts of code and set up nodes.

but such languages assume that their environment is already well-set-up;
that any external services they talk to are guaranteed to be there,
that language runtimes have already been configured on appropriate nodes,
etc.

but such languages assume that they're running in a specifically configured and prepared environment;


The "integration library" approach allows a distributed system to be treated holistically as a single program.
There are purpose-specific distributed languages which have the same goal,
but such languages assume the presence of a great deal of distributed systems infrastructure
that is maintained outside the language.
A program written in such a language is then only one component in a larger distributed system. 
The "integration library" approach allows, instead,
for one to write a program which manages *every* aspect of the distributed system above the hardware level,
such that the program and the distributed system really can be said to be the same thing.



These techniques, instead,
assume nothing about the environment,
and set it up from scratch...


by assuming almost nothing about


These techniques


but such languages assume that they are running on nodes that have already been configured,
and in general 


but they're generally substantially less expansive in their scope 
but they require using a different language,
and they generally are 
and they generally don't handle their own deployment
but they don't work with 
and they 

Collectively, these techniques replace
many services and 
significant 

significant amounts of code 

Collectively, these techniques replace large amounts of distributed system 
** ChoHag
*** my reaction
useless
** koo555
<koo555> catern, well, it says that you use some libs and typed python
to write service wrappers and a "service manager" to configure the sw
you build at your $job ... I had ran a friends homemade "service
manager" configured by yaml for some time ... i've checked out
something similar kinda unix-standard once ... i'm kinda familiar with
the concepts and some user-level usage of init scripts and systemd,
and i'm well aware of struggles for ..lets say ...unifying a
dev env with a production env .... my $dayjob sw is currently managed
by a python configurator/wrapper/generator for a docker stack/swarm
thing ... and i'm aware of the problem of making services wait for
each other, for example .. i reckon this is what "service mesh"
offerings also promise to tackle .. Anyway, generally i'm a fan of
"configuration as code" .. and i can only wish that one day we'll have
a neat declarative way to make something like your sys also
able to manage dockerized services meaningfully .. Your approach
obviously has limits, like, it doesn't actually route the messaging
between your services, and so can't, let's say, hold one service while
another is restarting ..  But the focus on typing is good..
*** my reaction
** sakasama
<sakasama> The examples are intuitive enough. I am unimpressed by the article, however, for a more fundamental reason.
<sakasama> The article is obviously intended to introduce and promote a tool for service management,
but I see no attempt to address the singular most important concern I have with such systems:
what actions are available to an operator when the automated logic fails?
*** my reaction
maybe I should mention REPLs?

UIs? debuggers? other interactive stuff?

yeah clearly I should...

OK I added a note in the nursery section.
I think that may be helpful.
** mzan
*** DSL
<mzan> catern: I will be sincere, the approach can be good (probably) but I think you have a rather confusing exposition style.
<mzan> I don't like the title, because your Python code is in the end another "DSL" one had to master for writing distribuited system.

*** introduction
<mzan> The introduction part is confusing in the begining.
<mzan> It starts making sense from "The integration libraries use normal programming language features, including Python 3's static type system, to handle distributed systems issues. For example, regular function arguments are used to express service discovery and dependencies between services; for each ser..."
<mzan> because one can figure out the scope of the tool.

*** TODO style fixes
<mzan> There are also minor style annoiance like
<mzan> "At $DAYJOB we have a sophisticated collection of libraries called "integration libraries". "
<mzan> You have "library called integration libraries" or you have "integration libraries".
<mzan> "a general-purpose language for running and customizing one distributed system component. "
<mzan> ^ usually one customize and then run a distribuited system.

# partially done, but should rework the first sentence still...
*** summary
<mzan> In the end, if I understood correctly, you are describing a devop tool, using Python as configuration and management language.
<mzan> Because you are using Python, you can code some services directly in Python, and so you can use Python + your tool also for describing directly services and launch them.
<mzan> But many times, you only configure and launch services written in other languages.

*** TODO style
<mzan> A minor detail, about your style, but sentences like these
<mzan> "There are many advantages of this style, but they all lead back to one thing: Being able to run the system flexibly, robustly, in a long-term maintainable way. "
<mzan> IMHO are very irritating...
<mzan> because you mention many advantages, but then you say there is only one, then you list many of them.
<mzan> A simple sentence like "The advantages of this approach are: ..." is better IMHO

*** DONE "style"
<mzan> Also about the style. The title says: "Write a distributed system as a single richly-typed directly-runnable program"
<mzan> ^ but in reality you are mainly configuring a distribuited system using a single program
<mzan> Because the majority of services are standalone applications.
<mzan> From the title it seems you are coding all the parts of the system.

*** DONE running
mzan: yes, that's one approach for sure, though there are others... although I'm not sure if you understand that the program here keeps running, it doesn't stop after "finishing deployment"
<mzan> catern: no, I thought to a single run program

*** TODO title
<mzan> catern: sorry if I can offend you, but I don't like your title.
"No more DSLs: Write a distributed system as a single richly-typed directly-runnable program"
<mzan> "No more DSLs" is a lie
<mzan> "richly-type", eh richly is rendundant, because no one will says "loosely-typed"
<mzan> and "directly-runnable"... are there "indirectly-runnable" programs?
<mzan> Too much adjective
> "No more DSLs" is not a lie, to call this a DSL is wrong and deeply confused
> I say in the article what "directly-runnable" means :)
> fair point about richly-typed though, I might be able to get away with just "typed"
<mzan> catern: maybe are the remote process you start, that are "directly-runnable"
<mzan> but "directly-runnable program" is as saying "a provable proof"
<mzan> "No more DSLs: Write a distributed system as a single richly-typed directly-runnable program"
<mzan> 5 adjectives, too much
<mzan> single, richly, typed, directly, runnable
<mzan> And "types" are not very important in your approach
<mzan> "No more DSLs: write a distributed system as a single, UTF8, commentable, richly-typed, directly-runnable program"
<mzan> Ansible: "No more DSLs: write a distributed system using a simple YAML configuration file"
<mzan> NixOS: "No more DSLs: write a distributed system using a real functional programming language designed exactly for this task"

*** my reaction 2

*** my reaction
ah he doesn't get that the program continues to run!

that's probably a major confusion, indeed...

maybe.. hmm...
maybe I need to talk more about async stuff...

because the fact that it's monitoring the system is maybe what's relevant...

argh, no, I just need to show a real example, I guess.
otherwise it's too unclear!

and that real example could also include some error handling...

no I need to say that the program continues to run early on.
It's kind of implicit in what I'm saying,
but I should say it explicitly.

yeaaaahhhhh hmmmmmmm

oh! I can say it in the same part as I say it's direct!

The resulting program is *direct*.
That is to say, it's not a DSL, and it's not "declarative";
it doesn't get interpreted by some tool and turned into a list of actions to perform.
It's interpreted by regular Python, and when the program calls =start_foo=,
it starts up the =foo= process, then and there.
When everything is started,
the program continue running, [[http://catern.com/supervisors.html][monitoring each process]]
and providing interfaces (the Python REPL, web UIs, etc)
to [[http://rsyscall.org/wish/][interact]] with the running system.

maybe this also answers how we interact with it?

and what about upgrades?

In the simplest design,
when the program starts, so does the distributed system,
and when the program stops, so does the distributed system;
in a deep sense, the program and the distributed system are the same thing.
(Additions to the design allow for upgrading the program without incurring downtime in the distributed system.)

nah...
we shouldn't mention that at all



when the program comes up, 

the lifespan of the distributed system and the lifespan of this program
are the same

Depending on the concrete design,
the running processe
killing the program might kill the running processes,


or it might keep them alive 


and notifying the user
and support 

When we 
** long discussion with sakasama and Corbin
okay so...
they're concerned about the operational aspects.

we got onto the "how do I upgrade it" issue,
which I guess was my own fault...

but anyway, I really need to say, early on, that the program conta
** andyc
see https://old.reddit.com/r/ProgrammingLanguages/comments/nqm6rf/on_the_merits_of_low_hanging_fruit/h0dpyb1/
*** my comment
Edit: To paraphrase your broader point though, you’re saying that this
only works for systems where one entity ultimately controls the entire
system, so for example this wouldn’t work for the internet at large
(but it would work for Google). Yes, that’s true. Two points on
that: 1. I think that applies to most systems, and 2. This is a
general pattern in many places, where there’s a larger “decentralized”
environment containing interacting centrally-controlled individual
entities: Economies contain firms, for example. I (probably) wouldn’t
want to centrally plan the entire economy, but centrally planning a
firm inside the economy is natural - it’s Coase’s theory of the
firm. For the same reasons, my system might interact with other
systems I don’t control, but I’d still want to be able to centrally
control my system - and it’s practical to do so.
*** DONE my reaction
    okay!

    so we're back on the issue of ownership...

    or of... central vs distributed control

    theory of the firm....

    It's important to note the key requirement for this approach:
    A single logical entity who controls/owns the entire syste


    wait no... you could imagine a single global repo declaring the internet,
    and you change the internet by sending PRs there.

    right so you *could* centrally control the internet,
    you just wouldn't want to.

    you *could* use internal markets inside your firm,
    you just wouldn't want to.

    hm

    This logically centralizes the control of the system,
    but that's already the case!

    right right,
    so... we already have logically centralized control (with infrastructure as code),
    we're just getting rid of the DSLs.

    in other systems where we don't already have logically centralized control, like the internet,
    we can't actually use this.
    we'd have to first logically centralize control!

    well okay we don't really have logically centralized control...
    we just have... the *ability* to logically centralize control...
    maybe I should look up how SDN talks about this?
    didn't find anything in the history article...

    It's valuable to distinguish which distributed systems this approach does and does not work for.
    Most distributed systems,
    such as those operated by a single company,
    have centralized authority;
    that is to say, ultimately the company (its owners and employees)
    has the authority to do anything they want to the distributed system;
    even though doing anything actually useful with the system might be [[https://www.lesswrong.com/posts/Wa2hASzbxyvutHJff/total-horse-takeover][hard]].
    A single-program system extends this to [[https://www.cs.princeton.edu/courses/archive/fall13/cos597E/papers/sdnhistory.pdf][logically centralized control]];
    that centralized authority is delegated to a single program which controls the system.
    Systems with centralized authority are easier to operate [[https://en.wikipedia.org/wiki/The_Nature_of_the_Firm][in many ways]],
    but such centralization isn't always possible;
    decentralized systems such as the Internet or the Bittorrent DHT
    contain many interacting subsystems, each with their own ultimate authority,
    so they couldn't be run as a single-program system.


    Other distributed systems are *decentralized*,
    such as the Internet or the [[https://en.wikipedia.org/wiki/Mainline_DHT][Bittorrent DHT]];
    there's no centralized authority over the system that can do anything it wants.
    So, obviously, that authority can't be delegated to a single program;
    the Internet couldn't be run as a single-program system.


    there's no centralized authority over the system that can do anything it wants.
    systems such as the 
    such centralization isn't possible
    
    and harder in others

    Centralized distributed systems interact with each other
    in the larger decentralized distributed system of the internet;
    
    
    benefitting from easier coordin
    the centralized distributed systems benefit from easier coordination with internal components,
    similar to how firms interacting in the larger decentralized economy,
    while avoiding internal transaction costs by being [[https://en.wikipedia.org/wiki/The_Nature_of_the_Firm][internally centrally planned]].



    So obviously
    There's no 
    There, a single-program system won't 

    To move to a single-program system is to create logically centralized control;
    that's a small 
change from a morally centralized control,
    to logically centralized point of control.

    have a logically centralized point of control - even if that isn't 
    A *decentralized* distributed system,
    such as the internet,
    has no central point of control.
** DONE the upgrade problem
   okay, people seem really preoccupied with this.

   I guess I gotta talk about this.
*** section
    Upgrading the distributed system is a matter of upgrading the single program.
    Doing this doesn't require downtime;
    one simple approach is to restart the single program without killing its child processes,
    and then the new version takes on the responsibility of gradually upgrading the child processes.
    More advanced techniques are applicable, too,
    such as [[https://en.wikipedia.org/wiki/Dynamic_software_updating][dynamic software updating]],
    to upgrade the single program without restarting it.
** HN comments
*** "this isn't distributed"
    huh???
*** this ignores all the other complexities of building a distributed system
    hm...

    maybe I should mention some of this???

I could say...
** lcapaldo
I found this interesting (along with a lot of the related posts and
others on the site) but it felt like it was being presented as an
alternative to something, but I had a hard time figuring out what that
something was. DSLs seems vague. I think I finally got the idea from
noticing the punny file name “caternetes.html”. The DSLs being
referred to are at least in part things like k8s configurations or
terraform etc.? Is that the idea? Analogously to the the post about
writing code not configuration?
*** mmmm
okay yeah fine,
let's stick something in the conclusion about what we're replacing.

terraform, kubernetes, etc...

hmm...
"DSLs seems vague"
maybe we can firm it up a bit even in the introduction?

I don't want to create ego threat at the beginning...

I think mentioning it in the conclusion is definitely better...

What are the DSLs that this replaces?
Kubernetes, Helm, Terraform, Ansible, systemd, cron jobs, shell scripts, configuration files in JSON, TOML, YAML, etc, etc.



* TODO HMmMmMm                                                     :noexport:
these people are not following my explanation of it as,
a framework for writing distributed systems.

maybe...
I shouldn't even talk about integration libraries?
perhaps I should just...
talk about it as an approach to writing a distributed system?
and just say, there are libraries for each service?

yeah that might be a good rework...

I can emphasize that this is regular Python code,
which forms a program,
which runs a distributed system...

or, maybe, emphasize that this is running with regular CPython.

but I need an approach name
** DONE approach name
I like "direct-style" but it's long

direct-style distributed systems

that's kind of not right though,
because... direct-style would be... directly accessing remote resources

well that's exactly what I'm doing actually.
just using subprocesses as optimizations...

direct-style distributed systems

hmmmmmmmmm

it's kind of like distributed languages...

central planning...

theory of the firm...

transaction costs to being distributed...

here is how to centrally plan a distributed system

central planning for distributed systems

No more DSLs: How to centrally plan a distributed system

software defined networking?

which are centrally planned networks...

software defined systems...?

I could mention SDN.

This is reminiscent of [[https://www.cs.princeton.edu/courses/archive/fall13/cos597E/papers/sdnhistory.pdf][SDN]],
which centrally controls the packet forwarding decisions made across many routers in a network,
although SDN is restricted to controlling networks.

blah...
maybe.. mebbee... i won't...
rework it...?

no, I have to. I have to come up with a name!!!

software-defined systems? SDS...

infrastructure as code...
that one sucks...

screw it!! direct-style!!

that's what I want to say!

direct-style systems.

direct-style... configuration.

direct-style systems...

I'll just say direct style

or... "direct systems"?

"direct deployment".

"central deployment"

it's not just central planning, too.
planning implies declarative
this is direct.
it's central doing.

central, direct, running.

direct-style system implementation

a distributed system in a single program...

single-program systems.

aha. single-program systems...

hmm, that's confusing too though...

toplevel systems...

"single-main systems" would be wrong since there are subprocesses with their own main...

"system toplevel" is what this program is.

single-toplevel systems

centrally programmed systems

centrally-defined systems

centrally-planned systems.

defined and planned are both confusing though because it's not planned!!

centralized systems...

not right either.

centrally is confusing, too, because it implies "centralized" as opposed to "distributed",
which might confuse people into thinking this has stuff to do with centralization...

single-program systems...

it's kind of catchy I guess...

and it does encapsulate what I'm going for, I guess...

yeah okay...
** new opening
Given a collection of component libraries,
one can write a distributed system as a single richly-typed program in a general-purpose language,
and run that program to [[http://catern.com/run.html][run the full system]],
for production or for testing the system [[http://catern.com/usertests.html][against its users]].
In this article, I describe the approach used to do this at $DAYJOB.
Although $DAYJOB's component libraries and single-program systems are proprietary,
they're built with open source libraries
which can be used today by anyone.
We write these libraries in Python because of its wide use,
but this approach is in no way tied to Python,
and this article should be comprehensible and useful to anyone interested in distributed systems.

First, each team at $DAYJOB maintains libraries for their components.
Some components are in Python,
but others are written in other languages (mostly C/C++/Java)
and run in a separate process.
In this way, performance-sensitive components of a distributed system can be written in faster languages than Python,
in the same way that a Fortran library can be used to accelerate math in Python programs.
All the component libraries are maintained in a single repository to reduce compatibility issues.





First,


and specific to the proprietary components in $DAYJOB's distributed system,





Components can be shared between systems in libraries,
allowing an entire faimily of programs to be written.
*** making it clear that multiple programs are created out of these libraries?
*** component libraries
SUCH a vastly better term!
*** alternative final introduction paragraph
This approach works for distributed systems with a single ultimate authority,
such as the organization operating the system,
not for systems with no central authority such as the Internet.
In a single-program system,
the single authority is delegated to a single progrm.
** even mentioning $DAYJOB
maybe I shouldn't?

it's maybe not positive...
except I guess it is useful to mention...

(I should really just do the controller thing at $DAYJOB first)
** subprocesses
One important thing is that regular commands are valid accelerated subprocesses... Components...
We rely on them to function in a distributed manner quickly.

using the nix-store binary for deployment is an example here
** things single-program systems already gives you/has for free (fuzzy)
- being able to run your system gives you the ability to avoid all the bad things listed in run.html
- usertests.html
- being self-contained means you can run anywhere without other teams breaking you, and without the need for further integration
  (!! that's a key point! the integration team will handle integrating, and remove the need for teams to do further integration work,
  with new daemons...)
  (that is - you don't need to port a daemon to run in a new place, and you can't be broken by service changse)
** enhancements/advantages/things one can build with single-program systems (fuzzy)
- linter
- a single coding style for diverse services
- typed interfaces for executables to make service-starting functions easier to write/generated,
  possibly incorporated with standard libraries;
  possibly you write the typed service-starting function interface and it generates your executable's main...
  but no, going from the executable first is more Flexible...
  but also harder to modify...
- remote execution of functions;
  maybe you just call into libraries from threads,
  perfect embrace of no-configuration, only code.
  (but what does that give you? well, see config.html)
- automatic post selection and load balancing
- service starting interface search
- maybe sandboxing instead of linting
- one box acting as init for other boxes (bringing them up and controlling them, fully)
