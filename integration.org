#+title: Write a distributed system as a single richly-typed directly-runnable program
#+HTML_HEAD: <style type="text/css">pre.src {background-color: #303030; color: #ffffff;} pre.src-python:before { color: #000000; }</style>
* Introduction
At $DAYJOB we have
a sophisticated collection of libraries called "integration libraries".
Each integration library contains richly-typed code in a general-purpose language
for running and customizing one distributed system component.
By using multiple such libraries together, one can write a distributed system as a single program,
and run that program to [[http://catern.com/run.html][run the full system]],
for production or for testing the system [[http://catern.com/usertests.html][against its users]].

# (link constructive proof post)
I describe these libraries here as a demonstration of
this language-focused, library-focused approach to writing distributed systems.
Although these integration libraries are proprietary,
and specific to the proprietary components in $DAYJOB's distributed system,
they're built on top of open source libraries
which can be used today by anyone.
Our integration libraries are all in Python because of its wide use,
but this approach is in no way tied to Python,
and this article should be comprehensible and useful to anyone interested in distributed systems.

# Although these libraries are proprietary, as are the components they run,
# so this article isn't as constructive as I would like.

Each team at $DAYJOB maintains integration libraries for their own components, but not their dependencies.
The integration libraries are maintained in a single repository to reduce compatibility issues.
Components are mostly services written in other languages, mostly C/C++/Java.
A typical Python integration library will use that foreign code by running it as a subprocess.

The integration libraries
use normal programming language features, including Python 3's static type system, to [[http://catern.com/progsys.html][handle distributed systems issues]].
For example, regular function arguments are used to express service discovery and dependencies between services;
for each service, there's a function which takes that service's dependencies as arguments,
starts that service up using those arguments,
and returns a value that can be passed as an argument (a dependency) to other service starting functions.
In general, services are "configured" by passing arguments, [[http://catern.com/config.html][in normal code]],
and multiple instances can be created just by calling functions multiple times with different arguments.

These service-starting functions directly run each service as a subprocess,
rather than delegating to a [[http://catern.com/supervisors.html][process supervisor]] service.
Functionality is shared through [[http://catern.com/services.html][libraries]] rather than by delegating to external orchestration systems,
making the integration libraries completely self-contained.
This allows the integration libraries to be used for deployment in any environment,
whether that's bare-metal, virtual machines, containers, or a single developer's machine.
# Maybe don't use TestCase boilerplate?
# nah I think that's good...
* Running a system with integration libraries
We'll see this in action with an example.
** orderd: an order entry daemon
- Accepts or rejects orders sent over a TCP listening socket
- Updates the =positiond= service with the positions
- Stores order data in a SQLite database

=orderd= is a real daemon ("d" is [[https://unix.stackexchange.com/questions/72587/why-do-some-linux-files-have-a-d-suffix][short for "daemon"]]), with a few details removed.
We're looking at =orderd= specifically
because it has only the three dependencies we've already mentioned.
Note that =orderd= itself is not necessarily written in Python.

For our example, we'll write a test of our distributed system.
We'll start up =orderd= and its dependencies (just =positiond=) for the test,
using functions from the integration libraries to run each service.
First, some testing boilerplate:
#+begin_src python
import unittest
from orderd import start_orderd

class TestOrderd(unittest.TestCase):
  def setUp(self) -> None:
    # TODO start up orderd and its dependencies
    self.orderd = start_orderd(...)

  def test(self) -> None:
    self.assertTrue("Do test stuff")
#+end_src

To write =setUp=,
we'll proceed by looking at the signature of the =start_orderd= function,
provided by the =orderd= integration library.

#+begin_src python
# in the "orderd" module
async def start_orderd(
  nursery: trio.Nursery,
  thread: rsyscall.Thread,
  positiond: positiond.Positiond,
  listening_sock: rsyscall.FileDescriptor,
  database: orderd.Database,
) -> Orderd:
#+end_src

We'll look at the =start_orderd= signature line by line,
creating each argument individually,
and at the end we'll call =start_orderd= and have a running instance of =orderd=.

The first three lines of the function signature 
(up to and including =thread: rsyscall.Thread,=)
are essentially common to all service starting functions.
The last four lines 
(starting with =positiond: Positiond,=)
are specific to =orderd=.
** =async def start_orderd(=
#+begin_src python
async def start_orderd(
#+end_src

=start_orderd= is an async function.
In Python, this simply means that it can run in parallel with other functions,
which allows us to start services up in parallel,
using Python-specific techniques which we won't show in this example.
Other than that, it's a completely normal function,
which is called with =await start_orderd(...)= from any other async function,
and which blocks execution until it returns.

Since =start_orderd= is async, we need to run it from an async runner.
We'll use the open source library [[https://trio.readthedocs.io/][=trio=]] for that,
which means we'll need to tweak our boilerplate slightly to use =TrioTestCase=.

#+begin_src python
from trio_unittest import TrioTestCase

class TestOrderd(TrioTestCase):
  async def asyncSetUp(self) -> None:
    self.orderd = await start_orderd(...)
#+end_src

Other than this change in boilerplate,
Python async functions work like any others;
you can safely ignore the "async" and "await" annotations.
We won't use any async features in this =TestCase= example;
the only use of async features will be later, with =start_exampled=,
when we look at how an integration library is implemented.
** =nursery: trio.Nursery,=
#+begin_src python
  nursery: trio.Nursery,
#+end_src

[[https://trio.readthedocs.io/en/stable/reference-core.html#trio.Nursery][=trio.Nursery=]] is defined by the open source [[https://trio.readthedocs.io/][=trio=]] library,
and it provides the ability to start up functions in the background.
We pass it in to =start_orderd=
so that =start_orderd= can start a function in the background
to monitor the running =orderd= process.
If the =orderd= process exits, the monitoring function will throw,
and the resulting exception will be propagated to the =trio.Nursery=,
which will deal with it in some way specific to how the =trio.Nursery= was produced.

In this case, we'll use =self.nursery= as provided by =TrioTestCase=,
which turns any failure in a background task into a failure of the whole test.

#+begin_src python
  async def asyncSetUp(self) -> None:
    # self.nursery provided by TrioTestCase
    self.orderd = await start_orderd(
      self.nursery,
      ...,
    )
#+end_src
** =thread: rsyscall.Thread,=
#+begin_src python
  thread: rsyscall.Thread,
#+end_src

[[http://rsyscall.org/rsyscall/][=rsyscall.Thread=]] is defined by the open source [[https://github.com/catern/rsyscall][=rsyscall=]] library,
and it provides the ability to run system calls, including running subprocesses.
We pass it in to =start_orderd=
so that =start_orderd= can start the =orderd= subprocess,
as well as perform other operations to prepare the environment for =orderd=.
An =rsyscall.Thread= may operate on a local or remote host,
or inside a container or VM, or on other kinds of nodes,
depending on how the =rsyscall.Thread= was produced,
but it provides a completely common interface regardless of where it runs.

Integration library code itself never runs distributed across multiple nodes;
there's a single Python interpreter on a single host.
All distributed operations are performed by method calls on =rsyscall.Thread= objects.

In this case, we'll use =local_thread= imported from =rsyscall=
and assigned to =self.thread=.
=local_thread= runs on the same thread as the Python interpreter - that is, on localhost.

#+begin_src python
from rsyscall import local_thread

  async def asyncSetUp(self) -> None:
    self.thread = local_thread
    self.orderd = await start_orderd(
      ..., self.thread, ...,
    )
#+end_src
** =positiond: Positiond,=
#+begin_src python
  positiond: Positiond,
#+end_src

This is the first =orderd=-specific argument.

=positiond= is a service which =orderd= updates with information about its position.
All the information required to connect to and use =positiond=
is contained in the =Positiond= class.

Since =positiond= is its own service, we need to use =start_positiond= to start it.

#+begin_src python
async def start_positiond(
  nursery: trio.Nursery,
  thread: rsyscall.Thread,
  workdir: rsyscall.Path,
) -> Positiond: ...
#+end_src

The first two arguments are shared with =orderd=.
The third argument, =workdir=, is unique to positiond.
=workdir= is a path in the filesystem that =positiond= will use;
in this case, =positiond= will use it
to store shared memory communication mechanisms and persistent data.

We'll pass a path in a temporary directory in this example.
#+begin_src python
    # Make a temporary directory
    self.tmpdir = await self.thread.mkdtemp()
    self.orderd = await start_orderd(
      ...,
      await start_positiond(self.nursery, self.thread, self.tmpdir/"positiond"),
      ...,
    )
#+end_src
** =database: orderd.Database,=
#+begin_src python
  database: orderd.Database,
#+end_src

This is a completely conventional SQLite database, initialized with the orderd schema.

#+begin_src python
    self.orderd = await start_orderd(
      ...,
      await orderd.Database.make(self.thread, self.tmpdir/"db"),
      ...,
    )
#+end_src
** =listening_sock: FileDescriptor,=
#+begin_src python
  listening_sock: FileDescriptor,
#+end_src

This is a listening socket,
passed down to =orderd= through file descriptor inheritance,
and used to listen for TCP connections.

This is standard Unix socket programming, so we won't go into this in depth;
although note that we create this with =self.thread=,
so that it it's on the same host as =orderd=.

#+begin_src python
  async def asyncSetUp(self) -> None:
    # Make a TCP socket...
    sock = await self.thread.socket(AF.INET, SOCK.STREAM)
    # ...bind to a random port on localhost...
    await sock.bind(await self.thread.ptr(SockaddrIn(0, "127.0.0.1")))
    # ...and start listening.
    await sock.listen(1024)
    self.orderd = await start_orderd(
      ..., sock, ...,
    )
#+end_src

** =) -> Orderd:=
#+begin_src python
) -> Orderd:
#+end_src

Like all good integration libraries,
=start_orderd= returns an =Orderd= class
which contains all the information required to connect to =Orderd=,
such as an address and port, a shared memory segment, or a path in the filesystem.

=start_orderd=, again like all good integration libraries,
will only return when the =orderd= communication mechanisms have been fully created,
and therefore the =Orderd= class can be [[http://0pointer.de/blog/projects/socket-activation.html][immediately used to connect]] to =orderd=.

** Full example
Here's the full, working example:
#+begin_src python
class TestOrderd(TrioTestCase):
  async def asyncSetUp(self) -> None:
    # self.nursery provided by TrioTestCase
    self.thread = local_thread
    self.tmpdir = await self.thread.mkdtemp()
    sock = await self.thread.socket(AF.INET, SOCK.STREAM)
    await sock.bind(await self.thread.ptr(SockaddrIn(0, "127.0.0.1")))
    await sock.listen(1024)
    self.orderd = await start_orderd(
      self.nursery, self.thread, 
      await start_positiond(self.nursery, self.thread, self.tmpdir/"positiond")
      await Database.make(self.thread, self.tmpdir/"db"),
      sock,
    )
#+end_src

Then we can proceed to [[http://catern.com/usertests.html][test by running user code]].

* Implementation of integration libraries
Now we'll step through an example of how an integration library is implemented.

This daemon is packaged and deployed with Nix;
at $DAYJOB we use a proprietary package manager with similar APIs.

Below is the full code for the =exampled= integration library,
with comments inline to explain it.
# Going line by line here in comments, rather than by arguments,
# because this is an implementation of an abstraction,
# not just a manipulation of abstractions

#+begin_src python
import nix_rsyscall
import rsyscall
import trio
# a Nix-specific generated module, containing the information required
# to deploy the exampled package; generated by setup.py.
import exampled._nixdep

class Exampled:
    def __init__(self, workdir: rsyscall.Path) -> None:
        self.workdir = workdir

async def start_exampled(
    nursery: trio.Nursery,
    thread: rsyscall.Thread,
    workdir: rsyscall.Path,
) -> Exampled:
    # deploy the exampled package and its dependencies; this doesn't deploy the
    # package for this Python library, but rather the exampled daemon
    package = await nix_rsyscall.deploy(thread, exampled._nixdep.closure)
    # build the command to actually run
    command = package.bin('exampled').args("--verbose", "--do-stuff-fast")
    # make the thread that we'll run that exampled command in;
    # this child_thread is a process under our control, see http://rsyscall.org
    child_thread = await thread.clone()
    # change the CWD of the child thread; CWD is inherited over exec, so it will be used by exampled
    await child_thread.mkdir(workdir)
    await child_thread.chdir(workdir)
    # exec the command in the child thread; this exec helper method returns a monitorable child process object
    child_process = await child_thread.exec(command)
    # monitor the child process in the background; see https://trio.readthedocs.io/
    # we'll get an exception if it exits uncleanly; this is our one use of async features.
    nursery.start_soon(child_process.check)
    # return a class containing exampled's communication mechanisms;
    # it communicates with the world only by creating files under `workdir'
    return Exampled(workdir)
#+end_src
** versions? :noexport:
versioning is controlled by which version of the integration library we use.

(I mean, that's the ideal, anyway...)
(actually that's true-ish since lnc_library works that way)
(but, we'll just omit any mention of versioning I think)
(we'll see what test readers think)
* Conclusion
# something about DSLs? and how this replaces them? Ansible, YAML, etc...

# In some sense, a program written with the integration libraries *is* a distributed system

# REPLs?
There are many advantages of this style,
but they all lead back to one thing:
[[http://catern.com/run.html][Being able to run the system]]
flexibly, robustly,
in a long-term maintainable way.

The techniques used in the integration libraries
are explained in greater detail in the posts linked in the introduction.
With those techniques,
and with the open source libraries [[https://github.com/catern/rsyscall][rsyscall]] and [[https://trio.readthedocs.io/][trio]],
anyone can write their own integration libraries for their own components,
and combine them with other libraries to run their distributed system in its entirety.
** list of things this replaces? :noexport:
systemd
ansible
kubernetes
hashicorp vault
* notes :noexport:
ok so we want the big example section still,
with the concrete code... hmmm....

we'll call it... toplevel?

yeah I still like the name toplevel

so the title can be something like...

Toplevel: A library for running


maybe I should say...

a library and style?

a style... for running distributed systems programmatically

coding standards?
an approach?

a way?
a path?
a paradigm?

paradigm is a bit pretentious

also the existing paper

a pattern???
I like pattern
a pattern for deploying distributed system programmatically

well!
I really am talking about the library of functions here.

I've already explained the pattern

I'm just talking about the library

i'm worried about my explanation style here...

maybe I should have the concrete examples to explain the theory,
instead of going through the dependencies one by one?

MAYBE I should just unite them?

Should I just extend the distributed systems article!??

Yeah! that actually would be good.


hmm it does make the article really long though...

maybe I should just cite it quickly, with the core point of,
expressing dependencies as arguments?
not the further examples of type parametrization and multiple environments?
yeah yeah that seems good.

so I can just reproduce that explanation


okay and so, we can show that we're looking at a test of orderd,
because it's completely self-contained; (a test is traditionally completely self-contained)
we don't have to posit any external hosts or nodes that we'll run orderd on,
we just do it totally self contained.

yeah,
"we could substitute more sophisticated values for thread and nursery here,
to get more sophisticated behaviors".


okay so:
- ???
- dependencies as arguments, link to "type systems for deploying distributed systems"
- ???
- "we'll look at a test because it's self-contained and a clean slate;
   we don't have to assume we have other multiple hosts that we'll use,
   and we don't have to worry about using persistent storage for data storage."
- example with "orderd"

So I guess I won't start by talking about tests;
we'll introduce tests later on as an example.

We'll start with a link to the distributed systems thing.
And also run your system. and code as config...

hmm.

or maybe not? we'll explain it more anecdotally;
at my job we have a library,
built along the lines blah blah,

or, wait. I want to say it concisely.
** post
At $DAYJOB we have 
a sophisticated collection of libraries for running components in our distributed system,
collectively called "integration libraries".
I describe them here as a constructive proof of (link constructive proof post)
the theory I've described elsewhere. (link each word)

- We use it to run our system, which is very important.
- A brief summary of the theory. [blah blah dependencies as arguments]

- The libraries consists of a collection of functions

** thoughts
okay so...
can we just say that we need to run our distributed system?

I mean, do we need to explain why we can't use other things?
maybe we don't need to do that, hm.

yeah I don't think we need this justification section,
which explains why we don't use kubernetes etc
(that can be... underdefined... and let people draw their own conclusions)

oho!
I can link "constructive proof" in the introduction,
to my new constructive proof article!

so no discussion of the justification versus other systems;
we'll just say, it's for running our system,
citing the "run your system" post.

well, kai says we should get to the point quickly.

I guess we can have one sentence about running the system...
at the start?
yeah and include a link to usertests too.

the key important prep is that dependencies are arguments;
we'll inline just that,
then we'll go into the actual example.

right so...

* title :noexport:
come up with a title!

Running a system with types in practice?

okay. so...

"Running a distributed system with "integration libraries""

or...

Running a distributed system programmatically

Infrastructure as code?

what actually is it?

it's an example of everything.

A real example of..

Running a distributed system, in practice

A concrete program which runs a distributed system

Distributed deployment with Python
Practical distributed deployment with Python
Concrete distributed deployment with Python

I should mention Python because that makes it clear these are real programs,
in a real language,
with real libraries.

Deploying a distributed system with Python
Examples of deploying a distributed system with Python
Examples of distributed system deployment with Python

we want to make sure that it's clear that the distributed system is not written in python

Using Python to deploy a distributed system
Examples of using Python to deploy a distributed system
An example of using Python to deploy a distributed system
Concrete usage of Python to deploy a distributed system
Concrete usage of a real language to deploy a distributed system
No more YAML: Using Python to deploy a distributed system


Yeah a tagline might be good

No configs: Using Python to deploy a distributed system

except it's not no configs,
that's what supernet does, it also uses python AFAIK,
the tricky part is...
well it just configures a bunch of services in a haphazard way,
with no guarantee that services are actually correctly configured.

Maybe, using *typed* Python?

Using typed Python to deploy a distributed system correctly
Using richly-typed Python to deploy a distributed system correctly
Using richly-typed Python to deploy a distributed system

hmmmm....

No more DSLs: Using richly-typed Python to deploy a distributed system

yeah that sounds kind of good.
but, we want to be a little more concrete.

like... hmm...

we want to express that it's a concrete example...
but I don't know if that's necessary?

I mean, my previous article was just saying,
use a type system.
and it had a python pseudocode example.

Oh how about:

No more DSLs: Using richly-typed Python to deploy a distributed system directly

well, I mean, that's just what we're doing I guess.
which I've outlined in previous articles, which I'll link...

yeah okay. I mean, this title integrates all those other articles.

and is concrete: this is Python, this is a distributed system.

what about:

Using richly-typed Python to deploy a distributed system directly
Using richly-typed Python to deploy a distributed system directly, no DSLs
Using richly-typed Python to deploy a distributed system directly, without DSLs

No I like the initial little tag, "No more DSLs".

and the article is the constructive proof;
linking to other articles to explain the theory.

okay, so that's a good preliminary title:

No more DSLs: Using richly-typed Python to deploy a distributed system directly

hmm it's kind of long

No DSLs: Using richly-typed Python to deploy a distributed system directly
No DSLs: Using typed Python to deploy a distributed system directly
No DSLs: Using typed Python to deploy a system directly
No more DSLs: Using typed Python to deploy a distributed system directly
No more DSLs: Using typed Python to deploy distributed systems directly
No more DSLs: With typed Python, deploy distributed systems directly
No more DSLs: Using typed Python to deploy distributed systems directly

yes, there we go:

No more DSLs: Using typed Python to deploy distributed systems directly

** explain no more DSLs?
I guess with that little "No more DSLs" tag,
I should maybe explain the alternative to Kubernetes...

that's probably another article yet again.

oh but I can just link the libraries instead of services article!

is there anything in there that isn't part of libraries vs services?

I guess open source exit vs voice, specifically the section about making forking easy,
that services make that hard.

but, still...
it's not just "No more DSLs",
it's also "No more orchestration/deployment services".

which is kind of implied in libraries vs services,
and also in the process superivors article.
yeah I don't think I need to explain that.
it's kind of related to SDN though.
** DONE make small excerpt explaining "direct" as mentioned above
I might have one excerpt though;
in addition to the "typed" paragraph,
I'll talk about "direct".
that should cover it, yeah.

and I can link the supervisors and libraries vs services articles there.

"instead of writing a library to generate configs to configure a service which deploys your services,
write a library to deploy your services."

"instead of making a service to deploy services, write a library to deploy services"
** maybe explain that we deploy on VMs, and so-called "bare metal"?
not containers?

I might mention that in the threads section...
but maybe not...
just might make it a little more explicit that we're mutating the Nix store?

well, we'll just see how beta readers take it.
* thoughts :noexport:
Should I justify writing a test?

- "we'll look at a test because it's self-contained and a clean slate;
   we don't have to assume we have other multiple hosts that we'll use,
   and we don't have to worry about using persistent storage for data storage."

but maybe not?

well, I don't have to justify it if I make a real example of running it elsewhere after that.

eh it's good enough

* TPS redesign :noexport:
  hey maybe I could merge the iqueue and the database?

  what I could do, maybe...
  is put the JSON after the delta message in the iqueue.

  I'm guessing posdelta won't mind that...

  just, that would be nice because,
  that would get rid of one of the stateful arguments


* TODO note the two advantages of passing dependencies as arguments :noexport:
  startup ordering and service discovery
* more notes :noexport:
maybe we should do the old style with an integration class?

actually the integration class might be confusing.
** TODO real prod example
we should probably have a real prod example though.
that will probably clear things up...

and prevent confusion...
** DONE link libraries vs services, that explains why not to use Kubernetes
link it in the links section, that is
* feedback :noexport:
** boopy
The writeup doesn't have much...
The writeup is kind of just explaining the code, no interpretation
*** formatting issues
White squares???? (fixed with =pre.src-python:before { color: #000000; }=)

Weird framing when you shrink the screen?
** tm007
*** DONE my reaction
it seems like he's concerned about compatibility?

the issues inherent in having this split across multiple libraries?

maybe I can just say there's one library...

or, say that they're maintained in a monorepo?

that might be a more direct way to say it...


okay I think I can add a line in the introduction saying,
"They're maintained in a single repository to reduce compatibility issues."

but this increase compatibility issues with your own components!
but, meh, maybe people are less likely to notice that.
** Logos01
*** my reaction
he's saying I'm not explaining the problem

that sounds kinda like tm007's initial confusion about what the point was

yeah okay that's fair and true.

tm007 was also asking about the domain...

maybe the conclusion should say, this approach is applicable everywhere.
and summarize again the links in the introduction;
maybe rewording them to talk more about the problem.

Logos01 didn't really read it tho...

maybe I should reword the second paragraph to emphasize the purpose first:
To run the system, for production or testing.
yes, that would be perfect.
** nedbat
<nedbat> i'll take a look
> thank you! much obliged, sorry for the repeated message heh. here it is, any feedback is appreciated: http://catern.com/integration.html
<nedbat> i think your introduction could use more of an introduction.
<nedbat> i had to wrap my head around the idea that there are interlinked essays here, and this piece claims to validate the others, but i haven't seen the others yet.
> let me say right away that they aren't necessary context, no need to look at them
<nedbat> that's what i would have thought, but you say this essay's whole point is to support those, sort of.
<nedbat> "First some unittest boilerplate for the test: "  what test? I thought we were starting up services. that could use some connective text.
<nedbat> you say that the async stuff isn't really relevant.  could you literally remove all the async parts, to truly focus on the parts you want to focus on?
<nedbat> "capability" is new to me, and I wonder how important it is to your points.
> (just want to say all this feedback so far is great, thank you)
<nedbat> is your conclusion something like, "Orchestrating services is often done with bash or dockerfiles, but you can use typed Python functions to achieve the same effect, with better testability"?
<nedbat> btw, capabilities might be something you still want to reference, but again you can do it in a way that gives people permission to skip over it if they want
> thanks, I'll just remove capabilities - you're right that it's irrelevant
> just to be clear, is that your understanding of the point of the article?
> ("is your conclusion something like")
<nedbat> catern: that's what I'm getting from it.
> yes, that's (a key of) the conclusion
> key part of*
<nedbat> if you put that statement in the introduction, it will help people understand where you are headed, and why you are showing them this
> yes, I'll definitely work on incorporating that and the rest of this feedback
> thanks again for reading it!
<nedbat> ok! :)
*** my reaction
great feedback, great...
** amogorkon
<amogorkon> catern, i would refrain from using "cute" names, why not full words?
> amogorkon: er, sorry, I don't know what you mean
<amogorkon> "orderd"
<graingert> isn't that a daemon that does orders
<amogorkon> positiond
> amogorkon: ah, I see, thanks for the feedback
<amogorkon> yw
*** my reaction
hmm yeah maybe this is true.

hMmmMMmmmMmmmm yeah it's not immediately familiar to people

so maybe I'll give them other names...

or just call it `order_daemon`

ohHH HmMMMM

they aren't really daemons though because they're running in the foreground... kinda...

hmmmmMMMMm
** corbin simpson
*** my reaction
he seems to not fully comprehend the dependencies-as-arguments approach



maybe I should clarify that there's no distributed Python execution going on?


(but Corbin can't understand anything. so...)


but yeah, definitely I should say that there's no distributed execution.

maybe in the rsyscall.Thread section?
** feep
<feep> catern: I kinda stopped reading because it seemed *inapplicable*
<feep> it seemed like the sort of article I'd read if I was interested in deploying services with python, and ... I'm not

...
<feep> catern: um, you misunderstand me, the problem wasn't the python
<feep> the problem is that I to a first approximation don't care about distributed systems.
> oops, very well
<feep> python probably doesn't put people off fwiw
<feep> or like ... distributed deployment, I guess, to put better
> nah I still think this is a good change
<feep> there's people into that sort of integration stuff, I'm just not one~
> feep: what if I was to include a line along the lines of "this is a new object-capability secure way to write distributed systems, like existing distributed languages such as Erlang and E but supporting any language"
<feep> I'm ... like, the problem isn't that I don't understand the reason why I should think this is cool
<feep> catern: you're just giving me more evidence why I don't find the topic of the article interesting :p
> feep: no no that's fine, you just corrected "I don't care about distributed systems" to "I don't care about distributed deployment", so what if I told you that it's really about the former, than the latter
<feep> oh hm
<feep> hm
<feep> idk, I just sort of bounced off
<feep> hm
<feep> catern: okay, I tentatively grant that this should maybe interest me, I just bounced off the topic around the time you started pulling in python libraries
> yes that's fine, it's kind of oriented towards someone who *does* care about distributed deployment right now, but that's not necessarily good :)
<feep> orient it towards people who care!
<feep> don't try to pivot to people who aren't interested, that kills articles (and browsers, cough firefox cough)
<feep> I feel the right way to do stuff is to fully commit to people who are interested back
*** my reaction
I'm guessing this is the title

and also the repeated mention of Python

maybe I should say, Python isn't important...

oh 
hm
it wasn't the python, it was the distributed deployment.

maybe I need to change the title

Writing distributed systems 

Writing distributed systems with 

Writing distributed systems without DSLs, without 

Write a distributed system as a single richly-typed directly-runnable program

yeah okay, much more focused on writing distributed systems now...
** dbohdan
<dbohdan> catern: Even after rereading, I have questions about your distributed system.  I know it isn't what the article is about, but I am left wanting more context.
> dbohdan: thanks for the feedback about the supervisors, and hmm I'll try and think of a way to deal with the code blocks...
> dbohdan: ah, thanks for the feedback. what questions do you have? and, if you don't mind, could you summarize very briefly your understanding of the article?
> (if you feel you can)
<dbohdan> catern: For example, are there multiple instances of each component in your system?
If there are, how do they interact/are they prevented from interacting?
Is this a something you deploy to physical servers, containers, or some cutting-edge cloud nonsense?
For me the deployment code is left hanging in the air without being able to imagine these things more concretely.
<dbohdan> "Prevented from interacting" is re: your use of SQLite
*** my reaction
great questions!!!

okay, so I should probably say explicitly we use this to deploy to... all three!
physical servers, virtual machines, containers, and cutting-edge cloud nonsense if you want it.

and, multiple instances...
we should express that different instances can be configured for different 
